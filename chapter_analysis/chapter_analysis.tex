% Much of these are still open because I haven't leveraged more recent (i.e. better) data.  So worth re-doing the analysis with it (I have better data & I'm better at coding & after writing this I see how it all fits together).

% Does it make sense to introduce methods (plus investigations, specifically for ACF/XCF) BEFORE describing the whole 'pipeline'?  This chapter is getting unwieldy, especially when I go back and forth between the mathematical methods.
\chapter{Analysis of oscillatory time series}
\label{ch:analysis}

% Introduction: overview of the 'pipeline'
\section{Overview}
\label{sec:analysis-overview}

% - Zielinski et al. (2014) is a good starting point

% From synthetic oscillations report
In the time series analysis literature, there is a wealth of analysis methods that aim to find parameters pertaining to the periodicity of noisy time series and that aim to characterise the relationship between two signals.
This is relevant for biological time series, especially because they are noisy due to both intrinsic and extrinsic noise sources, and because researchers often record multiple time series from the same system to study the relationship between such time series.
One example is the yeast metabolic cycle, which is known to be a metabolic oscillator that is coupled with the cell division cycle oscillator -- in this case, the two time series from this system consist of the level of metabolites to represent the metabolic oscillator and the activity of a component of the cell division cycle to represent the cell division cycle.

The problem with applying classical time series analysis methods to biological time series lies with two properties of biological time series: they are noisy and they are often short.
Classical methods such as Fourier analysis have been shown to be useful in characterising noisy biological time series when they are long, e.g. from neuron impulse recordings, which can include up to thousands of oscillations.
However, for examples such as the yeast metabolic cycle, it is not feasible to record time series that include such a high number of oscillations; in this case, 5-10 oscillations are more realistic.
As a result, methods like Fourier analysis only provide poor resolution for characteristics such as the period of oscillations.
% ----- end from synthetic oscillations report

\begin{figure}
  \centering
  \begin{subfigure}[htpb]{0.9\textwidth}
   \centering
   \includegraphics[width=\textwidth]{example_dataframe}
   \caption{
     Example dataframe
   }
   \label{fig:analysis-example-dataframe}
  \end{subfigure}
  \begin{subfigure}[htpb]{0.5\textwidth}
   \centering
   \includegraphics[width=\textwidth]{example_timeseries}
   \caption{
     Example time series
   }
   \label{fig:analysis-example-timeseries}
  \end{subfigure}
  \begin{subfigure}[htpb]{0.5\textwidth}
   \centering
   \includegraphics[width=\textwidth]{example_images}
   \caption{
     Example images
   }
   \label{fig:analysis-example-images}
  \end{subfigure}
  \caption{
    Data overview.
    The segmentation \& data analysis software \textit{aliby} (see section~\ref{sec:methods-segmentation}) produces a dataframe for each fluorescence channel --- \ref{fig:analysis-example-dataframe} shows the flavin signals for one particular experiment.
    Each row represents a cell and each column represents a time point.
    To illustrate the flavin signals, some sample time series are plotted from the dataframe in \ref{fig:analysis-example-timeseries} --- here, the horizontal axis shows time and the vertical axis shows the mean intensity of the signal in the cell, represented as numbers in the dataframe.
    Each data point derives from images, examples in \ref{fig:analysis-example-images}.
    As discussed in section~\ref{sec:methods-segmentation},
    image segmentation uses the brightfield images to define cell outlines, then \textit{aliby} overlays the cell outlines onto the fluorescent images to extract fluorescence intensity.
    Specifically, this intensity is the mean intensity of pixels within the cell outline, subtracted by the background intensity, and is thus the numerical values represented in each data element in the dataframe.
  }
  \label{fig:analysis-data-overview}
\end{figure}

To discuss the process of analysing oscillatory time series in more detail, I will be using my data as an example and divide this chapter according to steps in the process (see figure~\ref{fig:analysis-data-overview}).
My data consists of 100--1000 time series of recorded flavin intensity changes for each cell, indicating the yeast metabolic cycle.
Some experiments include HTB2::mCherry strain cells, and I obtain both flavin intensity changes and mCherry intensity changes from each cell;
the mCherry indicating the cell division cycle.
In these time series, there is a new time point every 5 minutes, for a total of 100--300 time points.
These data are stored as dataframes for each condition (strain and media conditions), with the time series as rows and time points as columns --
in the case of HTB2::mCherry cells, the flavin and mCherry time series are in separate data tables.

This chapter is divided into six sections, five of which correspond to a process and questions each process address:
\begin{enumerate}
  \item Cleaning: choosing data, filtering confounding trends, dealing with missing time points.
  \item Classification: is my time series oscillatory?
  \item Characterisation: I have one time series -- what properties does it have?
  \item Correlation: I have two signals from the same cell -- what are their relationships to each other?
  \item Clustering: I have many time series (of the same signal) from many cells -- what are their relationships to each other?
  \item Summary: bringing it all together in a pipeline.
\end{enumerate}

\section[Cleaning]{Cleaning: choosing data, filtering, missing time points}
\label{sec:analysis-cleaning}

The first step of any data science pipeline is cleaning data, and of course there is the adage: 80\% of data science is cleaning data [CITATION NEEDED].
Cleaning data is important because of the principle of `garbage in, garbage out' -- regardless of how cutting-edge the analysis methods are, if the input data is flawed, then you cannot get good insights from the data.
However, this step involves the most judgement calls, i.e. the analyst needs to decide on criteria to separate useful data from useless data.
These criteria may be based on existing standards of practice or, for systems biology, knowledge of either the underlying biological processes or of the mathematical constraints.
Often, these criteria can be arbitrary because there is an absence of existing standards of practice for the specific situation -- in this case, any decision is going to be a trade-off and needs justification.

Three problems I occur in my data are: choosing data, time series filtering, and dealing with missing time points.
They can best be illustrated in a heatmap of raw data from an experiment [DESCRIBE THE EXAMPLE EXPERIMENT] here (figure~\ref{fig:analysis-example-heatmap}):

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{example_dataframe_heatmap}
  \caption{
    Heatmap of example dataframe shown in figure~\ref{fig:analysis-example-dataframe}.
    Each row represents a cell, each column represents a time point, and the colour of the pixel (colour bar, right) represents the fluorescence intensity.
    Some cells are not present for the whole time course of the experiment, and errors in the segmentation pipeline produce missing time points seen as white pixels in the heatmap.
  }
  \label{fig:analysis-example-heatmap}
\end{figure}

The first step is choosing data.
Some time series have few time points.
Most often, these correspond to cells that were not present in traps for a long duration, and some can be the result of errors in the track merging algorithm used in the data extraction process [CAN I CITE ALAN'S THESIS?].
Here, I choose time series that have points present in at least 80\% of the total time points.
This is an arbitrary cut-off, but it ensures that my time series have enough oscillations for further analysis, such as characterisation of the frequencies of these oscillations.
Otherwise, short, and likely uninformative, time series can `pollute' algorithms that operate on the population of time series, confounding the analysis.

% Probably overkill?
%[FIGURE: SHOW LONG-TERM TRENDS.  DRAW A MEAN+STD ERR PLUS A COUPLE OF RAW TIME SERIES.]

Next, the data must be filtered to remove long-term trends.
Depending on the scientific question, such long-term trends can be useful:
for example, in climate science, if there is a time series of atmospheric \ce{CO2} over a period of decades, we would like to filter out the annual (seasonal) cycles and look at the long-term changes over time.
In the analysis of biological rhythms, however, long-term trends are the element that must be removed to uncover the periodic behaviour of biological time series \parencite{zielinskiStrengthsLimitationsPeriod2014} [OR IS THE 2022 PUBLICATION BETTER SUITED TO PROVE MY POINT?  IN ANY CASE, LIFT A COUPLE SENTENCES FROM THEIR DISCUSSION OF REMOVING TRENDS IN BIOLOGICAL TIME SERIES DATA].
As an example, \textcite{zielinskiPeriodEstimationRhythm2022} offers several methods of detrending time series: polynomial detrending (including linear detrending), sliding-window detrending, ... [READ THE PAPER AND INSERT THE REST HERE].
However, each method has its own caveats.

\begin{figure}
  \centering
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_raw}
   \caption{
     Raw signal
   }
   \label{fig:analysis-filter-raw}
  \end{subfigure}
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_butterworth}
   \caption{
     Signal processed with high-pass butterworth filter of frequency \SI[parse-numbers=false]{1/350}{\minute^{-1}}.
   }
   \label{fig:analysis-filter-butterworth}
  \end{subfigure}
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_slidingwindow}
   \caption{
     Signal processed by subtracting moving average with window size 30.
   }
   \label{fig:analysis-filter-slidingwindow}
  \end{subfigure}
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_savgol}
   \caption{
     Signal processed with Savitzky-Golay filter (window size 7, polynomial order 3).
   }
   \label{fig:analysis-filter-savgol}
  \end{subfigure}
  \caption{
    Different filtering methods result in different effects to both the signal (left panels) and the Fourier spectrum of the signal (right panels).
  }
  \label{fig:analysis-filter}
\end{figure}

In the process of finding a way to filter my time series, I have considered two methods: sliding-window methods (such as using a Savitzky-Golay filter) and a high/low-pass filter.
Sliding-window methods are common in the single-cell biological rhythm literature; for example... [EITHER (A) VOMIT A LIST OF CITATIONS AND LEAVE IT THERE, OR (B) MENTION A COUPLE AND DESCRIBE IN A BIT MORE DETAIL, GROUP SAVITZKY-GOLAY FILTERS, PUBLICATIONS THAT USE STUPIDER METHODS, ETC.  I'M SURE HEINEMANN'S PUBLICATIONS USE THEM, AND I'M SURE THERE WERE A COUPLE USED IN JOURNAL CLUBS THAT I'VE COMMENTED IN MY NOTES HAVING USED STUPID METHODS.]
However, the Savitzky-Golay filter unnecessarily smooths data
Subtracting a moving average relies on knowing a window size that approximates the expected oscillation period, gives no control over the signal frequencies, introduce artefacts in the frequency spectrum, and decreases the number of time points available (figure~\ref{fig:analysis-filter}).
Such methods may distort the data in a way that affects conclusions.
In contrast, defining a high- or low-pass filter offers direct control over frequencies, but the user must use a judgement call to define a critical frequency.
In my case, I define the critical frequency for a high-pass filter as \SI[parse-numbers=false]{1/350}{\minute^{-1}} as this corresponds to the upper limit of reasonable durations of yeast metabolic cycles and cell division cycles that I have observed in my single-cell microfluidics experiments.
Of course, this will exclude the possibility of metabolic cycles that go on for longer, but as I said, this is a trade-off I am willing to make so that I can extract the individual oscillations better.
For subsequent analysis, I use the high-pass Butterworth filter defined above to process raw fluorescence time series.

Next, missing time points can pose a problem, especially when many analysis methods [WHICH?] are not made for this.
In my case, these missing time points often arise because of errors in track joining [MORE ELABORATION PLEASE -- READ ALAN'S THESIS], but occur infrequently enough [EVIDENCE NEEDED] that I could just perform linear interpolation to fill in the data or discard the time series altogether.
However, there are methods that deal with missing time points.
Most notable is the Lomb-Scargle periodogram (LSP) -- to be discussed in section~\ref{sec:analysis-characterisation} -- which was developed for astronomical data that often has missing time points, corresponding to cloud cover, for example.
%Other methods that have been used in the biological time series literature include ... [FISH OUT SOME FROM ZIELINSKI ET AL. 2014, AND DESCRIBE THEM A BIT].

\begin{figure}
  \centering
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{snr_illustration}
   \caption{
     To define the signal-to-noise ratio, a cut-off frequency is defined to divide the area under the Fourier spectrum: signal, to the left of the cut-off (yellow), and noise, to the right (pink).
     The signal-to-noise ratio is thus defined as the signal area divided by the noise area.
   }
   \label{fig:analysis-snr-illustration}
  \end{subfigure}
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{pyruvate_snr_edit}
   \caption{
     The signal-to-noise ratio was computed for each processed signal within a dataset, and the distribution of these ratios within a dataset can indicate the overall quality of signals.
   }
   \label{fig:analysis-snr-histogram-example}
  \end{subfigure}
  \caption{
    The signal-to-noise ratio as a measure of signal quality.
  }
  \label{fig:analysis-snr}
\end{figure}

Finally, I discuss a way to evaluate the quality of a (processed) dataset -- specifically, computing a signal-to-noise ratio.
The signal-to-noise ratio compares the level of a desired signal to background noise and serves as a measure of quality in signal processing.
It takes various definitions depending on how the signal and noise are defined.
In my case, I base my definition on the frequency domain of the signal.
I define a cut-off frequency between signal and noise (figure~\ref{fig:analysis-snr}), based on the assumption that very high-frequency components of the periodogram correspond to noise and lower-frequency components correspond to the meaningful oscillations (and this is after filtering out the very low-frequency components that are the long-term trends I don't want).
Again, this is a judgement call, and this is based on looking at several experiments' mean Fourier spectrum and defining a number that best separates signal and noise [EVIDENCE NEEDED -- PERHAPS PLOTS FROM SEVERAL EXPERIMENTS THAT PRODUCE DIFFERENT-FREQUENCY OSCILLATIONS.  OR THE MEAN FOURIER SPECTRUM OF SOME EXPERIMENT, OVERLAID WITH THE CUT-OFF LINE.]

To summarise, choosing relevant and `clean' data is needed for all data pipelines, but time series data expected to be oscillatory needs filtering or detrending to separate the oscillations of interest from irrelevant trends in the data.
For my data, I choose cells whose tracks are present for at least 80\% of the experiment, interpolate missing time points, and use a high-pass Butterworth filter with a critical frequency of \SI[parse-numbers=false]{1/350}{\minute^{-1}} to remove long-term trends and to preserve noise for quality evaluation using the signal-to-noise ratio measure.

\section[Classification]{Classification: is my time series oscillatory?}
\label{sec:analysis-classification}

To assess how a perturbation affects the YMC, it is important to have a systematic method to determine whether a time series is oscillatory.
% However, there are challenges with classification of noisy biological time series with relatively few time points.
% In addition, such a classification task necessarily needs a cut-off somewhere, thus requiring judgement calls.
% You'd see this crop up in this section multiple times when I discuss my methods and I reveal different angles of approaching this.
Determining whether a time series is oscillatory, or rhythmicity detection, is technically difficult for several reasons.
From a signal processing perspective, any time-dependent signal can be decomposed into a combination of sinusoids of different frequencies.
Noise typically manifests as high-frequency components while trends manifests as low-frequency components --- the filtering methods in section~\ref{sec:analysis-cleaning} arise because of this fact.
It is thus more reasonable to detect the presence of a frequency within the range of interest.
However, this depends on knowing an expected range of frequencies.
When studying the circadian rhythm, this is often the case: studies expect rhythms of around \SI{24}{\hour} and \textcite{zielinskiStrengthsLimitationsPeriod2014} uses the range of \SIrange{16}{32}{\hour} for rhythmicity detection.
This method is less useful when the frequency of oscillations is unknown or known to be in a wide range of frequencies, as is the case for the yeast metabolic cycle.
Furthermore, there is no way to objectively specify a failure rate for a rhythmicity detection method as there is no independent method to estimate rhythmicity \parencite{zielinskiStrengthsLimitationsPeriod2014}, therefore such a classification method requires a subjective definition of whether each time series is oscillatory.
In other words, this is similar to the requirement of a training data set with human-defined labels, and thus a human-defined failure rate, for supervised machine learning.

In this section, I will discuss my use of spectral methods, model-fitting methods, and machine learning methods.

% Literature review subsection

% - Compare and contrast methods
% - Highlight challenges with large datasets of noisy biological data
% - Review existing methods first and then talk about the methods I tried, with results.
%   FFT (already copied from 10m, to be re-written slightly), AR (steal figures from AC22 poster & presentations made in that time)

\subsection{Rhythmicity detection using spectral methods}
\label{subsec:analysis-classification-spectral}

% Copied from 10m report
% ----------------------
% Minireview of studies about finding whether there is an oscillation in circadian time series (or other related time series) -- keep it relatively short.

% Discussion about oscillatory behaviour vs periodic behaviour?

\textcite{glynnDetectingPeriodicPatterns2006} describe a method to classify gene expression profiles as oscillating or non-oscillating based on the Lomb-Scargle periodogram \parencite{lombLeastsquaresFrequencyAnalysis1976}.
This periodogram was developed for time series with missing time points, and has a chi-square distribution that aids a statistical test for periodicity \parencite{scargleStudiesAstronomicalTime1982}.
The classification was based on controlling the false discovery rate for identification of oscillations.
In testing multiple hypotheses, the false discovery rate is defined as the proportion of cases in which the null hypothesis is true among all hypotheses in which the test is declared significant.
Increasing the false discovery rate thus increases the proportion of time series classed as oscillating.

I developed a classifier based on \textcite{glynnDetectingPeriodicPatterns2006} to classify time series of flavin autofluorescence as follows:

\begin{enumerate}
\item Let the data have $\mathcal{G}$ cells.
Let cell $g = 1, \dots{}, \mathcal{G}$ have a time series with $N_{g}$ time points.
The time series is thus denoted $Y_{g}(t) = y_{g}(t_{1}), \dots{}, y_{g}(t_{N_{g}})$.
\item For each time series, I define a range of test frequencies linearly from $\frac{1}{N_{g}}$ to the Nyquist limit (i.e. half the rate of image acquisition, \SI{0.2}{\minute^{-1}}, in this case).

With this definition, I compute the classical periodogram for each time series:
  \begin{equation}
    P_{g}(\omega) = \frac{N_{g}}{2\sigma^{2}} \left|\int_{-\infty}^{\infty} Y_{g} e^{-2\pi it}dt \right|, % check constants, etc
    \label{eq:normalised-periodogram}
  \end{equation}
        where $\sigma^{2}$ is the sample variance of $Y_{g}$.
        In this equation, the periodogram is normalised by the coefficient $N_{g}/2\sigma^{2}$ so that the area under the periodogram is constant across all time series.
        The Lomb-Scargle periodogram is equivalent to the classical periodogram if the time points are equally spaced \parencite{lombLeastsquaresFrequencyAnalysis1976}, as is the case for the vast majority of my data.
\item For each cell $g$, I denote the peak $h_{g} = \max_{j}P_{g}(\omega)$.
I define an effective number of independent frequencies $M = f_{max}N_{g}$ for each time series, where $f_{max}$ is the Nyquist limit \parencite{vanderplasUnderstandingLombScarglePeriodogram2018}.
  I then calculate the $p$-value of testing the null hypothesis that such a peak is due to chance:
  \begin{equation}
    p_{g} = 1 - (1 - e^{-h_{g}})^M
    \label{eq:lsp-pval}
  \end{equation}
  This formula is based on the exponential distribution of the power at a given frequency in the periodogram \parencite{scargleStudiesAstronomicalTime1982}.
\item I order the cells by $p$-values: $p_{(1)} \leq p_{(2)} \leq \dots \leq p_{(\mathcal{G})}$.
  To control the false discovery rate \parencite{benjaminiControllingFalseDiscovery1995}, I find $\hat{k}$ according to:
  \begin{equation}
    \hat{k} = \arg\max_{1 \leq k \leq \mathcal{G}}\{k : p_{(k)} \leq qk/\mathcal{G}\}
    \label{eq:lsp-khat}
  \end{equation}
  where $q$ is a defined false discovery rate.
\item Cells whose $p$-values correspond to $p_{(1)}, p_{(2)}, \dots, p_{(\hat{k})}$ are thus denoted to have statistically significant oscillatory behaviour for the false discovery rate $q$.
\end{enumerate}

%I then compared the classification results against manual classification of these time series into oscillating and non-oscillating.
The classifier was able to rank the time series by quality of oscillation (figure \ref{fig:ClassifierBestWorstTS}).
The peak of the normalised classical periodogram of each time series was used as a proxy for the quality of oscillation (figure \ref{fig:ClassifierBestWorstPS}).
By eye, birth events coincided with peaks of some higher-quality oscillations.
However, this was also true for some oscillations ranked as lower-quality.
% [COMMENTED -- don't think this sentence is really consequential, and the proposed plot doesn't add much] Additionally, higher-quality oscillations do not seem to be associated with imaging positions/flavin LED exposure times. % FIGURE: scatter plot, horizontal axis is rank, vertical axis is imaging position
% Wee bit of discussion (plus a plot to illustrate my point).  Deeper discussion about multiple main frequencies is in discussion, and has references to literature.
These oscillations were ranked as low-quality because the Fourier transform identified multiple main frequencies, and thus lowered the peak of the normalised periodogram.
Thus, a Fourier-based method may not adequately provide the information for a reliable ranking of oscillation quality.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{10m_ClassifierBestWorstTS}
  \caption{
    Classifier based on Lomb-Scargle periodogram ranks time series by quality of oscillation.
    Left column shows `best' five and right column shows `worst' five.
    Blue solid lines: flavin autofluorescence, red dashes: birth event automatically identified by \textit{BABY}.
  }
  \label{fig:ClassifierBestWorstTS}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{10m_ClassifierBestWorstPS}
  \caption{
    Highest peak of normalised classical periodograms is an inadequate proxy for quality of oscillations.
    Blue solid lines: normalised classical periodograms of time series in figure \ref{fig:ClassifierBestWorstTS}.
  }
  \label{fig:ClassifierBestWorstPS}
\end{figure}

The classifier has multiple caveats.
It one tuning parameter, the false discovery rate, and its value affects the proportion of time series classed as oscillating.
I resorted to subjective judgements of whether a time series exhibited oscillations, and thus any optimised false discovery rate would have low reliability.
A reliable ground truth is needed to optimise the value of the false discovery rate so that the classification accuracy is maximised.
Such a ground truth would ideally be from an experiment that is expected to give rise to time series that are not oscillatory.

\subsection{Rhythmicity detection using model fitting}
\label{subsec:analysis-classification-ar}

The autoregressive model has been used before to characterise biological time series \parencite{zielinskiStrengthsLimitationsPeriod2014}.
This model is based on the assumption that each data point in the time series can be expressed as a linear combination of $P$ data points that precede it --- $P$ thus representing the order of the model.
This autoregressive model thus `smooths' the time series.

\parencite{jiaFrequencyDomainAnalysis2021} describe an application of the autoregressive model, based on the fact that there is an analytical solution for the periodogram based on this model.
This publication describes a model of stochastic gene expression in a dividing cell, which predicts oscillatory gene expression.
Autoregressive models were fitted to simulated time series of oscillatory gene expression, and its parameters used to define a power spectrum for the time series.
I implemented this algorithm, described by \textcite{jiaFrequencyDomainAnalysis2020}, as follows:

\begin{enumerate}
  \item The algorithm relies on fitting a single time series $n(0), n(1), \ldots , n(M-1)$ with an autoregressive model $\mathrm{AR}(P)$ with order $P$:
        \begin{equation}
          \label{eq:ar-model}
          \phi_{0}n_{t} + \phi_{1}n_{t-1} + \phi_{2}n_{t-2} + \ldots + \phi_{P}n_{t-P} = \theta_{0}\epsilon_{t}
        \end{equation}
        where $\epsilon_{t}$ is a white noise satisfying $\langle \epsilon_{t} \rangle = 0$,
        $\phi_{0} = 1$, and
        $\phi_{1}, \ldots , \phi_{P}$ are real numbers such that the complex zeros of the polynomial $\Phi (z) = \sum_{k=0}^{P} \phi_{k}z^{k}$ lie outside the unit circle.
  \item The sample mean of the time series is estimated by:
        \begin{equation}
          \label{eq:ar-mean}
          \langle n \rangle = \frac{1}{M} \sum_{k=0}^{M-1}n(k)
        \end{equation}
  \item The sample autocorrelation function is estimated as:
        \begin{equation}
          \label{eq:ar-acf}
          R_{i} = \frac{1}{M} \sum_{k=0}^{M-1-i}(n(k) - \langle n \rangle)(n(k+i) - \langle n \rangle)
        \end{equation}
  \item The coefficients $\phi_{1}, \ldots , \phi_{P}$ are estimated by solving the Yule-Walker equation:
        \begin{equation}
          \label{eq:ar-yule-walker}
          \begin{bmatrix}
            R_{0} & R_{1} & \dots & R_{P-1} \\
            R_{1} & R_{0} & \dots & R_{P-2} \\
            \vdots & \vdots & \ddots & \vdots \\
            R_{P-1} & R_{P-2} & \dots & R_{0}
          \end{bmatrix}
          \begin{bmatrix}
            \phi_{1} \\
            \phi_{2} \\
            \vdots \\
            \phi_{P}
          \end{bmatrix}
          =
          \begin{bmatrix}
            R_{1} \\
            R_{2} \\
            \vdots \\
            R_{P}
          \end{bmatrix}
        \end{equation}
  \item The parameter $\theta_{0}$ is estimated as:
        \begin{equation}
          \label{eq:ar-noise-param}
          \theta_{0}^{2} = R_{0} - \sum_{k=1}^{P}\theta_{k}R_{k}
        \end{equation}
  \item The order $P$ is determined by minimising the Akaike information criterion:
        \begin{equation}
          \label{eq:ar-aic}
          \mathrm{AIC}(P) = \log \theta_{0}^{2}(P) + 2 \frac{P}{M}
        \end{equation}
        where $\theta_{0}(P)$ is the estimated $\theta_{0}$ (equation~\ref{eq:ar-noise-param}) for a specific $P$.
        In this step, $P$ is varied with $1 \leq P \leq 3 \sqrt{M}$, and the optimum order ($P$) is the one that gives the smallest value of $\mathrm{AIC}(P)$
   \item The power spectrum is thus estimated analytically using the parameters found in earlier steps by:
        \begin{equation}
          \label{eq:ar-power-spectrum}
          G(\xi) = \frac{1}{2 \pi} \cdot \frac{\theta_{0}^{2}}{|\sum_{k=0}^{P}\phi_{k}\me^{-ik\xi}|^{2}}, -\pi \leq \xi \leq \pi
        \end{equation}
        where $\xi$ represents frequency.
\end{enumerate}

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{jiaFrequencyDomainAnalysis2020_2ab_adapted}
  \caption{
    Power spectra (a) analytically derived from fitting an autoregressive model to time series (b) can be divided into four types.
    Type I lacks a local maximum and is denoted as lacking oscillations.
    Figure adapted from \textcite{jiaFrequencyDomainAnalysis2020}.
  }
  \label{fig:analysis-ar-classification}
\end{figure}

The resulting power spectra fell into four categories: one of which corresponded to a lack of oscillations, characterised by an absence of a local maximum in the power spectrum (fig~\ref{fig:analysis-ar-classification}).
This method thus makes it easy to compute the frequency of the oscillation from the location of the peak and quality of the oscillation from the height of the peak, as opposed to a Fourier spectrum computed base on noisy data.

\begin{figure}
  \centering
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{timeseries_example_for_ar}
   \caption{
     Example time series to illustrate the autoregressive model fitting method.
     The original time series is shown in light purple, and the autoregressive model with order 4 is fitted to produce the time series shown in dark purple.
     Birth events automatically defined by \textit{BABY} are shown in dashed vertical lines.
   }
   \label{fig:analysis-ar-timeseries}
  \end{subfigure}

  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{ar}
   \caption{
     The fitted model in~\ref{fig:analysis-ar-timeseries} leads to an analytical definition of a periodogram.
     The presence of peaks indicate that the time series is oscillatory.
     In this case, the peaks in the periodogram gives a good estimate of the most likely period of oscillation in the original time series.
   }
   \label{fig:analysis-ar-periodogram}
  \end{subfigure}
  \caption{
    Fitting an autoregressive model to determine period.
  }
  \label{fig:analysis-ar}
\end{figure}

Figure~\ref{fig:analysis-ar} shows an example of the autoregressive model method performed on my time series data of flavin autofluorescence.
In my investigations, very few time series within a dataset were classified as oscillatory using this method.
The problem is that there are no parameters that I can adjust to change the `tolerance' for detecting rhythmicity.
However, among the time series that it deems oscillatory, it guesses the frequency fairly well.

(Need: figures to summarise my investigation on population.  Probably need to re-do this on some datasets because it is poorly documented in my notes.)

Alternatively, I can treat the order of the model as a parameter, and perform model selection on it.

% This subsection needs better re-organisation.
% I've done some attempts by putting some headers in comments.
% But perhaps a better way is to draft some `summary' figures to show key takeaways, and then let the text flow accordingly.
\subsection{Machine learning approaches to classification}
\label{subsec:analysis-classification-ml}
% - Write results from classifier project (SVM, RF, etc.)

% Copied from org note: 'Using SVM with a population of cells', which isn't particularly well-written.
% ----------------------

% [TODO] Worth re-doing the whole experiment using better data, e.g. the data I'm actually using in the biological results chapter.

% OVERVIEW OF MACHINE LEARNING
% (needed to make the rest of the section make sense)
In machine learning, classification is defined as the process of identifying a category that a piece of input data belongs to.
In the context of this section, the classification task is identifying whether a time series (input data) is oscillatory (belongs to one category of two) or non-oscillatory (belongs to the other category of two).
When performing a classification task, input data is first converted to feature vectors in the process of featurisation.
This process uses domain knowledge related to the type or origin of the data to define characteristics of the data that may be useful for classification.
Each piece of input data has a label assigned to it to denote which category it belongs to.
The input data set is then divided into a training data set and a test data set.
The machine learning model is then fit on the training data set to fit parameters in the model, and then the performance of the model is evaluated on the test data set.
Such model evaluation is based on quantitative measures of how well the model matches data to appropriate labels.

% DESCRIPTION OF DATA

For this section, I used data from BY4741 cells under \SI{10}{\gram~\litre^{-1}} glucose, before any nutrient changes.
% Probably no need to go into so much detail.
I removed the first 25 time points that is most likely cells adapting to microfluidic conditions.
I also removed time points after time point 168, when nutrient switching occurred.
I removed all time series with missing time points.
In total, there are 294 cells, each with 118 time points, spaced 5 minutes apart.

% DATA PROCESSING

% [TODO] Repeat experiment with Butterworth filter, rather than sliding window detrending

% Figure ... shows a clear downwards trend (which I can't explain biologically yet). If an SVM model is trained using time points as features, there is a risk that the model recognises this trend rather than the presence of oscillations.
% This plot highlights the risk: for some reason non-oscillating time series (hand-scored) tends to have a smaller gradient.
% I thus detrended the data by generating a moving average with a sliding window size of 45 time points, and then dividing each data point by the moving average. I chose 45 time points because that covers ~3 cell division cycles.
% However, there are caveats.
% If the cell division cycle length differs, this window length should change; a spectral method (FFT, autoregressive) could give me an idea of what this cell division cycle length is.
% Additionally, this decreases the number of time points --- there are now 98.

The dynamic range of each time series are different from each other, so I additionally normalised each time series $x_{i}$ by computing the standard score:

\begin{equation}
  \label{eq:analysis-stdscore}
  z_{i,j} = \frac{x_{i,j} - \mu_{i}}{\sigma_{i}}
\end{equation}

where $i$ represents each time series, $j$ represents each time point which a time series, $\mu_{i}$ is the mean value of each time series across its time points, and $\sigma_{i}$ is the standard deviation of the values within each time series across its time points.
As a result, each normalised time series has a mean of 0 and a standard deviation of 1.

% LABELLING

Based on the processed data, I manually scored the 294 time series as non-oscillatory (label `0') or oscillatory (label `1').
Here, there are 211 oscillatory time series (72\%) and 83 non-oscillatory time series (28\%).
Thus, there is a slight class imbalance.

% SPLITTING

I chose 150 time series at random to be used as training data --- the rest becomes testing data.

% FEATURISATION

I explored three methods of featurisation: using time points as features, the time series' Fourier spectrum as features, and using \texttt{catch22} features \parencite{lubbaCatch22CAnonicalTimeseries2019}.

\texttt{catch22} is based on the \texttt{hctsa} toolbox \parencite{fulcherHctsaComputationalFramework2017} developed to compute vectors of features for time series.
This toolbox contains 7701 features defined from across the time series analysis literature.
I focused on the 22-feature \texttt{catch22} subset of the original 7701 features (table \ref{tab:catch22}).
\textcite{lubbaCatch22CAnonicalTimeseries2019} selected these 22 features because they minimise redundancy while maintaining classification performance across 93 test datasets.
This feature set excludes features that are dependent on mean or spread.

\begin{table}[htbp]
  \small
  \centering
  \begin{tabularx}{\linewidth}{bbS}
    \toprule
    Feature name & Description \\
    \midrule
    \texttt{DN\_\-HistogramMode\_\-5} & Mode of z-scored distribution (5-bin histogram) \\
    \texttt{DN\_\-HistogramMode\_\-10} & Mode of z-scored distribution (10-bin histogram) \\
    \texttt{SB\_\-BinaryStats\_\-mean\_\-longstretch1} & Longest period of consecutive values above the mean  \\
    \texttt{DN\_\-OutlierInclude\_\-p\_\-001\_\-mdrmd} & Time intervals between successive extreme events above the mean \\
    \texttt{DN\_\-OutlierInclude\_\-n\_\-001\_\-mdrmd} & Time intervals between successive extreme events below the mean \\
    \texttt{first\_\-1e\_\-ac} & First 1/e crossing of autocorrelation function \\
    \texttt{firstMin\_\-acf} & First minimum of autocorrelation function \\
    \texttt{SP\_\-Summaries\_\-welch\_\-rect\_\-area\_\-5\_\-1} & Total power in lowest fifth of frequencies in the Fourier power spectrum \\
    \texttt{SP\_\-Summaries\_\-welch\_\-rect\_\-centroid} & Centroid of the Fourier power spectrum \\
    \texttt{FC\_\-LocalSimple\_\-mean3\_\-stderr} & Mean error from a rolling 3-sample mean forecasting \\
    \texttt{CO\_\-trev\_\-1\_\-num} & Time-reversibility statistic, $\langle(x_{t+1} - x_t)^3\rangle_t$ \\
    \texttt{CO\_\-HistogramAMI\_\-even\_\-2\_\-5} & Automutual information, $m = 2, \tau = 5$ \\
    \texttt{IN\_\-AutoMutualInfoStats\_\-40\_\-gaussian\_\-fmmi} & First minimum of the automutual information function \\
    \texttt{MD\_\-hrv\_\-classic\_\-pnn40} & Proportion of successive differences exceeding $0.04\sigma$ \\
    \texttt{SB\_\-BinaryStats\_\-diff\_\-longstretch0} & Longest period of successive incremental decreases \\
    \texttt{SB\_\-MotifThree\_\-quantile\_\-hh} & Shannon entropy of two successive letters in equiprobable 3-letter symbolization \\
    \texttt{FC\_\-LocalSimple\_\-mean1\_\-tauresrat} & Change in correlation length after iterative differencing \\
    \texttt{CO\_\-Embed2\_\-Dist\_\-tau\_\-d\_\-expfit\_\-meandiff} & Exponential fit to successive distances in 2-d embedding space \\
    \texttt{SC\_\-FluctAnal\_\-2\_\-dfa\_\-50\_\-1\_\-2\_\-logi\_\-prop\_\-r1} & Proportion of slower timescale fluctuations that scale with DFA (50\% sampling) \\
    \texttt{SC\_\-FluctAnal\_\-2\_\-rsrangefit\_\-50\_\-1\_\-logi\_\-prop\_\-r1} & Proportion of slower timescale fluctuations that scale with linearly rescaled range fits \\
    \texttt{SB\_\-TransitionMatrix\_\-3ac\_\-sumdiagcov} & Trace of covariance of transition matrix between symbols in 3-letter alphabet \\
    \texttt{PD\_\-PeriodicityWang\_\-th0\_\-01} & Periodicity measure of \textcite{wangStructureBasedStatisticalFeatures2007}   \\
    \bottomrule \\
  \end{tabularx}
  \caption{\textit{catch22} features, adapted from \textcite{lubbaCatch22CAnonicalTimeseries2019}.
  }
  \label{tab:catch22}
\end{table}

After featurisation, to account for the different dynamic ranges of each feature, the features were scaled by computing the standard score, similar to equation~\ref{eq:analysis-stdscore}.

% MODELS

I explored several model architectures: a support vector machine (SVM) and a random forest (RF) architecture.

First, I used an support vector classifier (SVC) with these hyperparameters:
\begin{enumerate}
  \item A radial bias kernel.
  \item The kernel coefficient $\gamma = 1/N$, where $N$ is the number of features.
  \item The regularisation parameter $C = 1$.  The strength of regularisation is inversely proportional to $C$.
\end{enumerate}

Additionally, as a control, I randomly assigned time series to the oscillatory and non-oscillatory categories and used their time series as features.

I then evaluated the model using stratified five-fold cross-validation, using precision and recall as evaluation metrics.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{precision_recall}
  \caption{
    Evaluation of featurisation strategies for a support vector classifier to classify oscillatory time series of flavin autofluorescence.
    Precision and recall were used as evaluation metrics, and data points represent each of the five rounds of five-fold cross-validation.
  }
  \label{fig:analysis-precision-recall}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[htpb]{0.7\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_testing_featurevector_0}
   \caption{
   }
   \label{fig:analysis-svc-fft-0}
  \end{subfigure}

  \begin{subfigure}[htpb]{0.7\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_testing_featurevector_1}
   \caption{
   }
   \label{fig:analysis-svc-fft-1}
  \end{subfigure}
  \caption{
    Fourier spectra of cells in testing set classified as non-oscillatory (class 0,~\ref{fig:analysis-svc-fft-0}) and oscillatory (class 1,~\ref{fig:analysis-svc-fft-1}) by an SVC.
    Rows represent each cell, each used as an observation for the model.
    Columns represent each discrete frequency, used as a features for the model.
    Shades of red represent the power at each discrete frequency, used as feature values.
  }
  \label{fig:analysis-svc-fft}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{catch22_training_featurevector_mean}
  \caption{
    Mean values of each \texttt{catch22} feature across each cell classified as non-oscillatory (class 0) or oscillatory (class 1) by an SVC from the testing set.
    The horizontal axis shows each \texttt{catch22} feature from 0 to 21, and the vertical axis shows the mean value of each feature across cells within an identified class.
  }
  \label{fig:analysis-svc-catch22}
\end{figure}

Using time points as features, the metrics show that precision and recall is better than random, and stratified five-fold cross-validation does not suggest overfitting (figure~\ref{fig:analysis-precision-recall}).
Using the Fourier spectrum as features, precision and recall are comparative as with using time points as
features.
However, there seems to be greater variation in these measures.
The spectra (figure~\ref{fig:analysis-svc-fft}) suggest that oscillatory cells exhibit a peak, corresponding to features 6--7, that is absent in non-oscillatory cells.
The peak corresponds to a period of 70--80 minutes.
This peak may explain the good precision \& recall scores.
Nevertheless, precision and recall are best with \texttt{catch22}.
Feature vectors seem to suggest that the \texttt{FC\_LocalSimple\_mean3\_stderr} feature (represented as feature 9 in figure~\ref{fig:analysis-svc-catch22}) plays an important role in distinguishing the oscillatory and non-oscillatory time series.
This feature is defined as the mean error from a rolling 3-sample mean forecasting.

[CONFIRM IMPORTANCE VIA RECURSIVE FEATURE ELIMINATION, USING RANDOM FOREST?]

% ADAPTATION OF SVM: PREDICT PROBABILITIES

% Include result from randomly assigning labels as a control?
% This should give a `hill' in the middle.
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{classifier_histogram_probabilities_adapted}
  \caption{
    Histogram of probabilities of whether a time series in the test data set is classified as oscillatory by the SVC.
  }
  \label{fig:analysis-svc-proba-histogram}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{proba_00}
    \caption{
      0\%
    }
    \label{fig:analysis-svc-proba-00}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{proba_21}
    \caption{
      21\%
    }
    \label{fig:analysis-svc-proba-21}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{proba_38}
    \caption{
      38\%
    }
    \label{fig:analysis-svc-proba-38}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{proba_61}
    \caption{
      61\%
    }
    \label{fig:analysis-svc-proba-61}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{proba_88}
    \caption{
      88\%
    }
    \label{fig:analysis-svc-proba-88}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{proba_99}
    \caption{
      99\%
    }
    \label{fig:analysis-svc-proba-99}
  \end{subfigure}
  \caption{
    Test set time series shown by probability that each is oscillatory, as determined by the SVC.
  }
  \label{fig:analysis-svc-proba-gallery}
\end{figure}

Using the \texttt{catch22} features, I extended the support vector classifier enabling probability estimates, so that the model computes the probability that a time series is oscillatory, rather than assign one of two outputs.
Figure~\ref{fig:analysis-svc-proba-histogram} show that the distribution of probabilities falls in a U-shape, and this is good.
In addition, these probabilities can also serve as a score for the quality of oscillations, as illustrated by figure~\ref{fig:analysis-svc-proba-gallery}.

[MORE EVALUATION? e.g. ROC curve, precision-recall curve.  MAYBE IT IS OVERKILL HERE.]

In sum,
% I mention modularity clustering before its appropriate section here.
% I can create a reference to that section, or move this paragraph to that section and re-word it a bit so it fits the logical flow.
% ------------------------------------
the \texttt{catch22} features seem to function best so far.
%It makes sense -- given that I could plot cells in \texttt{catch22} feature space and perform modularity clustering, an SVM model based on drawing a `lane' between two clouds of points in \texttt{catch22} feature space should work.
%Furthermore,
%\textcite{lubbaCatch22CAnonicalTimeseries2019} has tested the features' ability to categorise time series across multiple large datasets.
A simple machine learning architecture is sufficient in classifying noisy biological time series into oscillatory and non-oscillatory categories, complete with probabilities.
This method was also able to identify which time series feature was the most important in distinguishing between oscillatory and non-oscillatory time series.

\section[Characterisation]{Characterisation: I have one time series -- what properties does it have?}
\label{sec:analysis-characterisation}

The importance of characterising my time series is that I can quantify how my yeast metabolic cycles respond to genetic and nutrient perturbations.
Here, I discuss characterising periods, phases, and amplitudes of the oscillations.
%Note that I discuss many of the same non-machine learning methods as I did for the classification section -- such methods are often developed for characterisation but offer features that are useful for classification, but it makes more sense in my thesis to order it this way because you'd usually try to filter out the non-oscillatory time series before trying to grab properties of the oscillatory ones.
Then I will discuss the merit of combining several methods, given the limitations of analysing noisy biological time series that I have found.
% [THIS SENTENCE MAY GO]
Characterisation is also intimately related to classification, in particular the machine learning methods: these methods can be adapted to tell apart different strains (presumably of different shapes) rather than telling apart oscillatory and non-oscillatory time series.

% Literature review subsection
% STEAL IDEAS FROM: ZIELINSKI ET AL. 2014
\subsection{Periods, phases, amplitudes}
\label{subsec:analysis-characterisation-quantities}

% Adapted from meeting with Zielinski.
% TODO with this:
% - support with evidence, e.g. discussion points from his paper and others

% PERIOD
The period of an oscillatory time series is the easiest characteristic to compute a fixed number for because it is well-defined.

% SHAPE
The shape of oscillations in a time series is difficult to analyse and assign a meaningful quantitative measure to --- it is easier to describe the shape by eye.
In the signal processing point of view, the shape of an oscillation is defined by a set of frequencies of sinusoids and their respective power values.
In other words, to quantitatively describe the shape of an oscillation, a vector of values is needed, and any fixed-number representation necessarily removes some information about the shape.
Alternatively, the shape can be expressed in terms of skewness, treating one oscillation as if it were a probability distribution [CITATION NEEDED].

% PHASE
There are challenges in obtaining a fixed number of represent the phase of an oscillator time series based on the signal.
Most importantly, the phase must be defined with respect to a human-defined reference, which may not be clear depending on the data.
In addition, the phase angle can have uncertainty, particularly if the time series is noisy.
There are period estimator methods [ELABORATE HERE], but they will always give an error range.
Estimating the phase angle also requires multiple replicates as estimation methods rely on plotting populations of time series and seeing if they overlap.
Modular arithmetic is also needed for such methods.
In sum, more information is needed to estimate the phase compared to other methods.
When a quantitative representation of the phase is obtained, it can be expressed in absolute terms (in units of time), or in relative terms with respect of the period (in radians).

% AMPLITUDE
The amplitude of the time series is easiest to compute and easiest to explain.
Methods to estimate the amplitude relies on fitting a cosine to the signal.
Such methods are adequate if the shape of the time series does not change by much, but are inadequate if oscillations are skewed, as the cosine function models symmetrical oscillations.
However, the amplitude is likely the least important characteristic to study in the biological timekeeping field.

Of these, finding the period is the easiest to do and most pertinent for my investigation of the yeast metabolic cycle, so I'm going to focus on this.
\textcite{zielinskiStrengthsLimitationsPeriod2014} discuss four methods: FFT NLLS (insert full name here), MFourFit (insert full name here), MESA (insert full name here), and the Lomb-Scargle periodogram.

% Copied from meeting with Zielinski.
% I probably need to try out at least the first two
% TODO: Convert to prose
\begin{description}
    \item [FFT NLLS]
        \begin{itemize}
            \item Commonly used in the circadian rhythm field, and therefore a good first-try.
            \item Enforces shapes.  Chooses number of sinusoids that fit the shape best, up to 5.  Then reports the best component.
            \item Has confidence intervals, e.g. relative amplitude error ($\leq$ 50\% is good).
            \item Skewness does not affect period.
        \end{itemize}
    \item [MFourFit]
        \begin{itemize}
            \item Uses harmonics.  Can use the harmonics to describe shapes of oscillations.
            \item Fits only 1 component, unlike FFT NLLS.  Period and phase interpreted from the main sinusoid component.
            \item Enforces shapes.
            \item Skewness does not affect period.
        \end{itemize}
    \item [MESA]
        \begin{itemize}
            \item Fit autoregressive model then gets period from it.
            \item Scours periods, then selects model that does best.
            \item Empirically does well and fast.
            \item Is based on a very different principle than MFourFit, so if both methods agree on the value of the period, then we can be confident that we have the correct period.
        \end{itemize}
    \item [Lomb-Scargle periodogram]
        \begin{itemize}
            \item Fits one sinusoid, assumes a period.
            \item Choose one that gives least error.
        \end{itemize}
\end{description}

For further detail, I discussed MESA in section~\ref{subsec:analysis-classification-ar} and the Lomb-Scargle periodogram in section~\ref{subsec:analysis-classification-spectral}.

Another common period-estimation method relies on the autocorrelation function, and it was used in [INSERT SOME PUBLICATIONS HERE].
Here, I am going to discuss adapting the autocorrelation function for my data.

\subsection{Autocorrelation function}
\label{subsec:analysis-characterisation-acf}

In this section, I show that the autocorrelation function can be adapted to characterise the period and noise properties of populations of both sinusoid time series and time series and asymmetrical oscillations.
I first do so with synthetic time series with known properties and adapt the methods to my data that exhibit similar oscillations as the synthetic time series.

\subsubsection{Mathematical definitions}
\label{subsubsec:analysis-characterisation-acf-maths}

The cross-correlation function used in this chapter is adapted from \textcite{pietschDeterminingGrowthRates2023}, as follows:

\begin{enumerate}
  \item Let the data have $M$ cells.
        Each cell $i$ in the population of $M$ cells has a time series $x_{1}^{(i)}, \ldots x_{N}^{(i)}$ of quantity $x$ and a time series $y_{1}^{(i)}, \ldots y_{N}^{(i)}$ of quantity $y$.
        Let both time series have a sampling interval of $\Delta t$.
  \item The deviation from the population mean for each time series is computed:
        \begin{equation}
          \delta x_{t}^{(i)} = x_{i}^{(i)} - \frac{1}{M} \sum_{j}x_{t}^{(j)}
          \label{eq:xcf-dmeans-x}
        \end{equation}
        \begin{equation}
          \delta y_{t}^{(i)} = y_{i}^{(i)} - \frac{1}{M} \sum_{j}y_{t}^{(j)}
          \label{eq:xcf-dmeans-y}
        \end{equation}
  \item Based on \textcite{kivietStochasticityMetabolismGrowth2014}, the cross-covariance of the two time series $x$ and $y$ at a time lag of $r\Delta t$, is thus given by:
        \begin{equation}
          C_{xy}^{(i)}(r\Delta t) =
          \begin{cases}
            \frac{1}{N-r} \sum_{t=1}^{N-r} \delta x_{t}^{(i)} \cdot \delta y_{t+r}^{(i)} & \text{if } r \geq 0 \\
            C_{yx}^{(i)}(-r \Delta t) & \text{if } r < 0
          \end{cases}
          \label{eq:xcf-xcov}
        \end{equation}
    \item The cross-correlation is thus given, with normalising by the standard deviation, by:
        \begin{equation}
          R_{xy}^{(i)}(r \Delta t) = \frac{C_{xy}^{(i)}(r \Delta t)}{\sqrt{C_{xx}^{(i)}(0) C_{yy}^{(i)}(0)}}
          \label{eq:xcf-xcf}
        \end{equation}
\end{enumerate}

The autocorrelation of a time series $x$ is thus the cross-correlation of the time series with itself, i.e.\ $R_{xx}^{(i)}(r \Delta t)$.

[TODO: CHECK IF CONSISTENT WITH CODE, AND ALSO ACCOUNT FOR MEAN OVER TIME OPTION]

To model time series, I choose the harmonic oscillator and the FitzHugh-Nagumo oscillator because they are composed of simple equations with few parameters, well-characterised, and mimic the biological time series that I am interested in.
The harmonic oscillator models the sinusoidal shape of flavin autofluorescence oscillations.
The FitzHugh-Nagumo model \parencite{fitzhughImpulsesPhysiologicalStates1961} is a well-characterised relaxation oscillator originally developed to model excitable physiological systems, and gives asymmetrical oscillations.
This oscillator models the shape of histone 2B abundance patterns that indicate the progress through the cell division cycle \parencite{garmendia-torresMultipleInputsEnsure2018}.

The harmonic oscillator $y$ can be defined by:

\begin{equation}
  \sndif{y}{t} = -\omega^{2}y
  \label{eq:harmonic}
\end{equation}

% First-order DEs redundant?
or as a system of first-order differential equations:

\begin{equation}
  \begin{aligned}
    \ndif{y}{t} &= v \\
    \ndif{v}{t} &= -\omega^{2}y
  \end{aligned}
  \label{eq:harmonic-1o}
\end{equation}

where the sole parameter $\omega$ represents the angular frequency.
The solution $y(t) = A \sin(\omega{}t + \phi)$ --- where $A$ is a constant representing amplitude and $\phi$ is a constant representing phase, both determined by initial conditions of the system --- defines a sinusoidal time series.

The FitzHugh-Nagumo model is defined by in a system of first-order differential equations:

\begin{equation}
  \begin{aligned}
    \ndif{v}{t} &= v - \frac{v^3}{3} - w + RI_{\mathrm{ext}} \\
    \tau \ndif{w}{t} &= v + a - bw
  \end{aligned}
  \label{eq:fhn}
\end{equation}

where, in the context of the model being developed to model neuronal impulses:
\begin{itemize}
  \item $v$ represents the membrane voltage, and
  \item $w$ represents the linear recovery variable;
\end{itemize}

and the parameters are $RI_{\mathrm{ext}}$ (external stimulus), $\tau$, $a$, and $b$.

The solution $v(t)$ is taken as a model for histone 2B abundance patterns (figure~\ref{fig:fitzhughnagumo_sample}).

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{fitzhughnagumo_sample.png}
  \caption{
    Sample time series based on the solution $v(t)$ of the FitzHugh-Nagumo model, with parameters
    $RI_{\mathrm{ext}}$ = 0.4, $\tau$ = 12.5, $a$ = 0.7, $b$ = 0.82.
  }
  \label{fig:fitzhughnagumo_sample}
\end{figure}

Biological time series, and certainly the ones that I study, have noise.
So, it makes sense to add noise to our models as well, because noise affects the behaviour of models and the analysis methods applied to the time series generated by our models.

I use two types of noise in my investigation: Gaussian noise and Gillespie noise.
Gaussian noise is the simple, first-approach case, and is generated by randomly drawing samples from the normal distribution $\mathcal{N}(0,\sigma^{2})$, where $\sigma$ denotes the standard deviation of the distribution and thus controls the size of the noise.
Gillespie noise emulates noise from biological systems and is generated using the direct method of the Gillespie algorithm \parencite{gillespieExactStochasticSimulation1977} on the birth-death process model.

The Gillespie algorithm was developed for stochastic simulations of a biochemical system.
Consider such a system with $M$ reactions $R_{1}, \ldots , R_{j}, \ldots R_{M}$ involving $N$ species $S_{1}, \ldots , S_{i}, \ldots S_{N}$ in a fixed volume $V$ at thermal equilibrium.
Let $X_{i}(t)$ represent the number of molecules of $S_{i}$ at time $t$, and the state vector

\begin{equation}
  \mathbf{X}(t) \coloneqq [X_{1}(t), \ldots , X_{N}(t)]
  \label{eq:gillespie-statevector}
\end{equation}

thus gives the state of the system at any given time $t$.

Each reaction $R_{j}$ is described by two quantities:
\begin{enumerate}
  \item A state-change vector $\mathbf{v}_{j} \coloneqq [v_{1,j}, \ldots , v_{N,j}]$ which defines how the stoichiometry of the system changes if the reaction occurs.
        $v_{i,j}$ represents the change in the stoichiometry of $S_{i}$ when $R_{j}$ occurs.
  \item A propensity function $a_{j}$, which gives the probability, given a the state $\mathbf{X}(t) = \mathbf{x}$, that one $R_{j}$ reaction occurs in the volume $V$ within the following short time interval $[t, t+dt)$.
        % TODO: Verify this expression -- this is from Mona, but this exact form doesn't appear in Gillespie (1977), Gillespie (2007), or Wilkinson (2018).
        This function is defined by
        \begin{equation}
          a_{j}(\mathbf{x})dt \coloneqq k_{j} \prod_{n=1}^{N}\mathbf{v_{n}}S_{n}
          \label{eq:gillespie-propensity}
        \end{equation}
        where $k_{j}$ is the rate constant of reaction $R_{j}$.
\end{enumerate}

The Gillespie algorithm aims to estimate the state vector given the initial state $\mathbf{X}(t_{0}) = \mathbf{x}_{0}$.
It does so by iteratively choosing the next reaction that occurs based on its probability, and choosing its firing time based on a probability distribution.
Combining these simulations gives a trajectory of state vectors across the time course of interest.
In detail, the direct Gillespie algorithm can be defined as \parencite{gillespieStochasticSimulationChemical2007}:

\RestyleAlgo{ruled}
\begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{Stochastic model (with species $S_{1}, \ldots , S_{i}, \ldots S_{N}$ and reactions $R_{1}, \ldots , R_{j}, \ldots R_{M}$, along with a state-change vector $\mathbf{v_{j}}$ and a rate constant $k_{i}$ for each reaction $R_{j}$); initial time $t_{0}$, and initial model state $\mathbf{X}(t_{0}) = \mathbf{x}_{0}$}
  \KwOut{Trajectory of state vectors $\mathbf{X}(t)$, with $t$ taking discrete values in $[t_{0}, t_{\mathrm{max}}]$}
  \While{$t < t_{\mathrm{max}}$}{
    Calculate the propensities $a_{j}(\mathbf{x})$ based on the current state $\mathbf{x}$\;
    Calculate the combined propensity $a_{0}(\mathbf{x}) = \sum_{j}a_{j}(\mathbf{x})$\;
    Generate two random numbers $r_{1}$ and $r_{2}$, both from the uniform distribution $U(0,1)$\;
    Choose the next reaction $R_{j}$ with $j$ given by the smallest integer that satisfies $\sum_{j^{\prime}}^{j}a_{j^{\prime}(\mathbf{x})} > r_{1}a_{0}(\mathbf{x})$\;
    Calculate the time to the next reaction $\tau = \frac{1}{a_{0}(\mathbf{x})}\ln(\frac{1}{r_{2}})$\;
    Simulate the next reaction by updating the state vector $\mathbf{x} \leftarrow \mathbf{x} + \mathbf{v_{j}}$ and store the new vector in $\mathbf{X}(t)$\;
    Update the time by $t \leftarrow t + \tau$ and store the new time\;
  }
  \KwRet Trajectory of state vectors $\mathbf{X}(t)$ for a vector of times $t$\;
  \caption{Direct method of the Gillespie algorithm}
  \label{alg:gillespie}
\end{algorithm}

The birth-death process is a common, simple stochastic model used for the modelling of gene expression.
The model describes a species that is produced at a linear birth rate $k_{0}$ and destroyed at a linear death rate $d_{0}$, and is defined by the system of equations in~\ref{eq:birth-death-process}:

\begin{equation}
  \begin{aligned}
    R_{1}: \varnothing \ce{ &->[k_{0}] P}\\
    R_{2}: \ce{P &->[d_{0}]} \varnothing
  \end{aligned}
  \label{eq:birth-death-process}
\end{equation}

\begin{figure}
  \centering
  \begin{subfigure}{0.9\textwidth}
    \centering
    \includegraphics[width=\linewidth]{gillespie}
    \caption{
      Stochastic simulation of trajectory of $P$, as defined by the birth-death process with $k_{0} = 5$ and $d_{0} = 0.05$ (equation~\ref{eq:birth-death-process}), produced by the direct Gillespie algorithm.
      $t_{\mathrm{max}}$ in the simulation is \num{1500}, but the time points were then interpolated on a regular grid of \num{1000} time points.
    }
    \label{fig:gillespie_trajectory}
  \end{subfigure}

 \begin{subfigure}{0.9\textwidth}
    \centering
    \includegraphics[width=\linewidth]{gillespie_noise_samples}
    \caption{
      Sample trajectories of Gillespie noise, produced from the latter half of trajectories as those shown in~\ref{fig:gillespie_trajectory}.
    }
    \label{fig:gillespie_noise_samples}
  \end{subfigure}

  \caption{Generation of Gillespie noise.}
  \label{fig:gillespie_noise}
\end{figure}

To produce Gillespie noise, a stochastic simulation employing the direct method of the Gillespie algorithm was performed on the birth-death process model with defined $k_{0}$ and $d_{0}$ parameters.
The final time was defined in such a way that allows the trajectory of the amount of $P$ over time to reach a steady state (figure ...).
This time varied depending on the $k_{0}$ and $d_{0}$ values, but the final time of \num{1500} was chosen as it was long enough to have the trajectory each steady state for the $k_{0}$ and $d_{0}$ values used in this study.
The latter half of the trajectory was taken and then put on a grid with \num{1000} regularly-spaced time points, equal to the number of time points for the oscillators (harmonic and FitzHugh-Nagumo).
The time series was then normalised by subtracting the mean ($k_{0}/d_{0}$) and then dividing by $\sqrt{1/d_{0}}$ to create a time series representing Gillespie noise with mean 0 and standard deviation $\sqrt{k_{0}}$.
This Gillespie noise thus has a standard deviation of noise amplitude $A = \sqrt{k_{0}/d_{0}}$ and noise timescale $\tau = 1/d_{0}$ --- in other words, the rate parameters of the birth-death process control the noise properties of this Gillespie noise.

Both Gaussian and Gillespie noise were added to the synthetic oscillatory time series using element-wise sums.

\subsubsection{Effect of noise parameters on the autocorrelation function}
\label{subsubsec:analysis-characterisation-acf-sinusoid}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
  % TODO: Replace with 3 sinusoids
    \includegraphics[width=\linewidth]{sinusoids_outofphase}
    \caption{
      400 out-of-phase sinusoids with frequency 0.03.
    }
    \label{fig:acf-sinusoids-nonoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{sinusoids_outofphase_acf_corrected}
    \caption{
      Autocorrelation function of~\ref{fig:acf-sinusoids-nonoise-ts}.
    }
    \label{fig:acf-sinusoids-nonoise-acf}
  \end{subfigure}

  % TODO: Use the (high) noise level quoted, and generate 3 sinusoids to replace this,
  % as above.
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{noisysinusoids_outofphase}
    \caption{
      400 out-of-phase sinusoids with frequency 0.03, each with Gaussian noise with a standard deviation of 3.
    }
    \label{fig:acf-sinusoids-gausnoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{verynoisysinusoids_outofphase_acf}
    \caption{
      Autocorrelation function of~\ref{fig:acf-sinusoids-gausnoise-ts}.
    }
    \label{fig:acf-sinusoids-gausnoise-acf}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p05_mean}
    \caption{
      Sample sinusoids with frequency 0.03, each with Gillespie noise with $k_{0} = 5$ and $d_{0} = 0.05$.
    }
    \label{fig:acf-sinusoids-gillnoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p05_acf}
    \caption{
      Autocorrelation function of~\ref{fig:acf-sinusoids-gillnoise-ts}.
      Red line is defined by $y = e^{-2d_{0}T}$, where $T$ represents lag, which should in theory fit the median autocorrelation function.
    }
    \label{fig:acf-sinusoids-gillnoise-acf}
  \end{subfigure}

  \caption{
    Effect of type of noise on the autocorrelation function.
  }
  \label{fig:acf-sinusoids}
\end{figure}

Need to make sure that we understand the autocorrelation function, so we start from the simplest case: the sinusoid.
Want to understand what processes control the shape of autocorrelation and cross correlation functions.

As the underlying dynamic process is stationary with a constant mean, we can modify our calculation of the autocorrelation function so that the mean is calculated over time and replicates.
Random initial phase sampled from the distribution \(Unif[0,2\pi)\).

Autocorrelation functions of out-of-phase sinusoids resemble a cosine wave with an amplitude of 1, which is how they should be in theory.
As a check, each oscillation of the autocorrelation function corresponds to the period of the source oscillations.

To emphasise the effect of noise, here I repeat the analysis with Gaussian noise with standard deviation 3.0.
Here, the amplitude of the autocorrelation functions are decreased and the variation between each time series' autocorrelation function is increased.
And at higher lag times, this variation is greater because there is less data that is used.

Simulate Gillespie noise using a birth-death process, then add to oscillator.
I performed 100 replicates of a sinusoid of frequency 0.03 with different phases.
I then generated 100 trajectories to Gillespie noise as described above, and added them together to produce the simulated replicates.
This gave the following mean across replicates and autocorrelation function.
As a check, I drew an exponential decay function (\(y = e^{-2d_{0}T}\), where \(T\) represents lag) to the autocorrelation function.
The exponential function should fit the median autocorrelation function.
In addition, the oscillations in the autocorrelation function should occur every period of the sinusoid, as already shown above.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p5_mean.png}
    \caption{
      Sample sinusoids, with Gillespie noise ($k_{0} = 5$, $d_{0} = 0.5$).
    }
    \label{fig:acf-noisetimescale-highd0-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p005_mean.png}
    \caption{
      Sample sinusoids, with Gillespie noise ($k_{0} = 5$, $d_{0} = 0.005$).
    }
    \label{fig:acf-noisetimescale-lowd0-ts}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p5_acf.png}
    \caption{
      Autocorrelation function of~\ref{fig:acf-noisetimescale-highd0-ts}.
    }
    \label{fig:acf-noisetimescale-highd0-acf}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p005_acf.png}
    \caption{
      Autocorrelation function of~\ref{fig:acf-noisetimescale-lowd0-ts}.
    }
    \label{fig:acf-noisetimescale-lowd0-acf}
  \end{subfigure}

  \caption{
    Effect of death rate ($d_{0}$) of Gillespie noise on the autocorrelation function.
    Red lines on bottom figures (autocorrelation functions) are defined by $y = e^{-2d_{0}T}$, where $T$ represents lag.
  }
  \label{fig:acf-noisetimescale}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{acf_fit_example.png}
    \caption{
      Exponential decay functions of the form $y = (1-C)e^{-DT}+C$, with $C$ and $D$ as variable parameters, were fit to the mean autocorrelation function, the peaks of this mean function, and the troughs of these functions, using non-linear least squares.
    }
    \label{fig:acf-noisetimescale-effect-fit}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{deathrate_vs_decay.png}
    \caption{
      The exponential fitting was performed across a range of $d_{0}$ while $k_{0}$ was held constant at 5.
      The decay rates $D$ of the exponential fits scale linearly with $d_{0}$.
    }
    \label{fig:acf-noisetimescale-effect-relationship}
  \end{subfigure}

  \caption{
    Characterising the autocorrelation function to quantify the effect of Gillespie noise timescale.
  }
  \label{fig:acf-noisetimescale-effect}
\end{figure}

Varying timescale of noise.

Higher death rate seems to decrease the decay timescale for the autocorrelation function.
Lower death rate seems to introduce long-term trends in the simulated signals.
It also increase the decay timescale for the autocorrelation function and increases the variation between autocorrelation functions between replicates.
In theory, the decay rate $D$ should scale linearly with the death rate $d_{0}$.  Sweeping across values of $d_{0}$, I confirm that is the case.
This figure thus summarises the effect of death rate in decay timescale.


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k1_d0p05_mean.png}
    \caption{
      Sample sinusoids, with Gillespie noise ($k_{0} = 1$, $d_{0} = 0.05$).
    }
    \label{fig:acf-noiseamplitude-lowk0-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k25_d0p05_mean.png}
    \caption{
      Sample sinusoids, with Gillespie noise ($k_{0} = 25$, $d_{0} = 0.05$).
    }
    \label{fig:acf-noiseamplitude-highk0-ts}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k1_d0p05_acf.png}
    \caption{
      Autocorrelation function of~\ref{fig:acf-noiseamplitude-lowk0-ts}.
    }
    \label{fig:acf-noiseamplitude-lowk0-acf}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k25_d0p05_acf.png}
    \caption{
      Autocorrelation function of~\ref{fig:acf-noiseamplitude-highk0-ts}.
    }
    \label{fig:acf-noiseamplitude-highk0-acf}
  \end{subfigure}

  \caption{
    Effect of birth rate ($k_{0}$) of Gillespie noise on the autocorrelation function.
    Red lines on bottom figures (autocorrelation functions) are defined by $y = e^{-2d_{0}T}$, where $T$ represents lag.
  }
  \label{fig:acf-noiseamplitude}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.45\linewidth]{birthrate_vs_ydispl.png}
  \caption{
    Quantifying the effect of Gillespie noise amplitude on the autocorrelation function.
    Exponential decay functions of the form $y = (1-C)e^{-DT}+C$, with $C$ and $D$ as variable parameters, were fit to the mean autocorrelation function, the peaks of this mean function, and the troughs of these functions, using non-linear least squares (as in~\ref{fig:acf-noisetimescale-effect-fit}).
    The exponential fitting was performed across a range of $k_{0}$ while $d_{0}$ was held constant at 0.05.
    The y-displacements $C$ of the exponential fits to peaks and troughs converged to 0 as the noise amplitude $k_{0}/d_{0}$ increased, showing that the amplitude of the oscillations in the autocorrelation function decreases as the noise amplitude increases.
  }
  \label{fig:acf-noiseamplitude-effect}
\end{figure}

Varying amplitude of noise.

Lower birth rate decreases the amplitude of noise.
It also makes the autocorrelation function more robust and decreases the variation between replicates.
Higher birth rate increases the amplitude of noise.
It also makes the autocorrelation function less robust and increases the variation between replicates.
Similar to previously, I fit \(y = (1-C)e^{-kT}+C\), using non-linear least squares, to the mean autocorrelation function, the peaks of this mean function, and the troughs of these functions.
To show that the amplitude of the oscillations in the autocorrelation function decreases as the birth rate increases, I plotted the fitted \(C\) (y-displacement) parameters against the noise amplitude \(k_{0}/d_{0}\).

If a population of replicate oscillatory time series is modelled with the sum of sinusoids and Gillespie noise, then the birth rate and death rate can control the shape of the autocorrelation function.  The death rate controls the timescale of noise and thus how fast the autocorrelation decays as lag increases.  The birth rate controls the amplitude of noise and thus controls how robust the autocorrelation function is.  Knowing these relationships, one can deduce noise parameters from the autocorrelation functions of real signals.

Gillespie noise seems to model the noise I observe in experiments better than white noise, and I can even tune the parameters to create a better fit.

Take home: parameters from fitting can be used to obtain noise properties.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{26643_ts.png}
    \caption{
      Sample time series of flavin autofluorescence.
    }
    \label{fig:acf-sinusoid-biol-ts}
  \end{subfigure}

  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fy4_26643_plots_06.png}
    \caption{
      Autocorrelation function across a population of time series of flavin autofluorescence.
    }
    \label{fig:acf-sinusoid-biol-acf}
  \end{subfigure}

  \caption{
    Using the autocorrelation function to characterise oscillations in flavin autofluorescence.
  }
  \label{fig:acf-sinusoid-biol}
\end{figure}

My intention is to have it model oscillations of flavin fluorescence that act as a proxy for the yeast metabolic cycle.

\subsubsection{The autocorrelation function extended for use with FitzHugh-Nagumo oscillators}
\label{subsubsec:analysis-characterisation-acf-fhn}

% TODO: Fish for figures.
% They aren't in the version of the report that I used to seed this section,
% but they may be in a more updated version of the report, or in some random org note.
% Failing that, I'm sure there are figures in org resources.
% Hopefully I won't have to regenerate them.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_meanplot}
    \caption{
      Sample FitzHugh-Nagumo oscillators generated with parameters $RI_{\mathrm{ext}}$ = 0.4, $\tau$ = 12.5, $a$ = 0.7, $b$ = 0.82.
      Each was added with Gillespie noise generated with parameters $k_{0} = 5$ and $d_{0} = 0.05$.
    }
    \label{fig:acf-fhn-gillnoise-ts}
  \end{subfigure}

  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_acf}
    \caption{
      Autocorrelation function of~\ref{fig:acf-fhn-gillnoise-ts}.
      Red line is defined by $y = e^{-2d_{0}T}$, where $T$ represents lag.
    }
    \label{fig:acf-fhn-gillnoise-acf}
  \end{subfigure}

  \caption{
    The autocorrelation function of FitzHugh-Nagumo oscillators with Gillespie noise.
  }
  \label{fig:acf-fhn}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_expofit}
    \caption{
      Exponential decay functions of the form $y = (1-C)e^{-DT}+C$, with $C$ and $D$ as variable parameters, were fit to the mean autocorrelation function, the peaks of this mean function, and the troughs of these functions, using non-linear least squares.
    }
    \label{fig:acf-fhn-noiseparams-fit}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_highnts_expofit}
    \caption{
      Similar to figure~\ref{fig:acf-fhn-noiseparams-fit}, but with a greater noise timescale, showing the limitations of this fitting with data using the FitzHugh-Nagumo oscillator as a base.
    }
    \label{fig:acf-fhn-noiseparams-fit-highnts}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_deathrate_vs_decay.png}
    \caption{
      The exponential fitting was performed across a range of $d_{0}$ while $k_{0}$ was held constant at 5.
      The decay rates $D$ of the exponential fits scale linearly with $d_{0}$.
    }
    \label{fig:acf-fhn-noiseparams-noisetimescale}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_birthrate_vs_ydispl.png}
    \caption{
      The exponential fitting was performed across a range of $k_{0}$ while $d_{0}$ was held constant at 0.05.
      The y-displacements $C$ of the exponential fits to peaks and troughs converged to 0 as the noise amplitude $k_{0}/d_{0}$ increased, showing that the amplitude of the oscillations in the autocorrelation function decreases as the noise amplitude increases.
    }
    \label{fig:acf-fhn-noiseparams-noiseamplitude}
  \end{subfigure}

  \caption{
    Quantifying the effect of Gillespie noise parameters on the autocorrelation function of FitzHugh-Nagumo oscillators.
  }
  \label{fig:acf-fhn-noiseparams}
\end{figure}

Autocorrelation function, lag axis scaled by period.
Note slightly different shape of oscillations here compared to sinusoid.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.9\textwidth}
  \centering
  % TODO: Edit out the flavin bits
    \includegraphics[width=\linewidth]{htb2mCherry_26643_plots_purple_01.pdf}
    \caption{
      Sample time series of histone 2B abundance (pink).
    }
    \label{fig:acf-sinusoid-biol-ts}
  \end{subfigure}

  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{htb2mCherry_26643_plots_mCh_06.png}
    \caption{
      Autocorrelation function across a population of time series of histone 2B abundance.
    }
    \label{fig:acf-sinusoid-biol-acf}
  \end{subfigure}

  \caption{
    Using the autocorrelation function to characterise oscillations in histone 2B abundance.
  }
  \label{fig:acf-sinusoid-biol}
\end{figure}

My intention is to have it model periodic changes in histone 2B intensity levels and yeast cells progress through the cell division cycle.

\subsubsection{Summary}
\label{subsubsec:analysis-characterisation-acf-summary}

My investigation aims to address a signal processing question.  Therefore, for the purposes of my investigation, it is not important that these oscillators function as an accurate model of the biological systems responsible for the biological oscillations observed.  Indeed, given that up to 50 flavoproteins may be responsible for flavin autofluorescence, a component of such a poorly biochemically characterised system like the yeast metabolic cycle, it is not feasible to find a mathematical model that accurately describes the biological oscillations.

Discussion: FitzHugh-Nagumo oscillation is `skewed', therefore some methods quoted by \textcite{zielinskiStrengthsLimitationsPeriod2014} will not work.

So long as the time series resemble the real oscillators of interest, the hope is that the conclusions from investigating our simple \& well-characterised models may answer questions on the behaviour and relationships between our real oscillators. [this sentence is a bit vague, but will revisit this after I weave together the story]  Potentially, this investigation can be extended to oscillators modelled by other systems of differential equations that describe other biological rhythms.

\subsection{Combining methods to get a picture of periodicity}
\label{subsec:analysis-characterisation-combined}

As each method comes with own advantages and limitations -- especially apparent when we're dealing with noisy biological data -- it is useful to use several methods in concert get an idea of the periodicity (or other properties) of the oscillations.
For example, \textcite{potvin-trottierSynchronousLongtermOscillations2016} combines the autocorrelation function and the Fourier transform to study the changes in the periodicity of a modified model of the repressilator.
The autocorrelation function, in particular, has been used in the yeast metabolic cycle literature, e.g. \textcite{papagiannakisAutonomousMetabolicOscillations2017}.

Here, I consider combining the autocorrelation function and the Fourier transform, and will use this in my analysis of biological data.

% [FIGURE: SHOWS A GOOD TIME SERIES, FFT, ACF, AR, AND THE MOST PROBABLY OSCILLATION FREQUENCY FROM EACH]

% [FIGURE: THE SAME AS ABOVE BUT LOOKING AT A POPULATION OF TIME SERIES.  THESE TRAJECTORIES SHOULD HAVE BEEN COMPUTED INDIVIDUALLY FOR EACH TIME SERIES.  THE CAPTION SHOULD EXPLAIN THE MEAN/MEDIAN AND ERROR RANGES.]

\section[Correlation]{Correlation: I have two signals from the same cell --- what are their relationships to each other?}
\label{sec:analysis-correlation}

So far, we have only considered one type of time series from the yeast metabolic oscillator -- the flavin signal.
However, the yeast metabolic oscillator and the cell division cycle form part of a system of coupled oscillators; certainly, in my experiments I record signals that monitor both.
Investigating correlations between two paired time series, originating from the same cell, leads to an understanding of the relationship between the two oscillators.
The most important being the phase relationship: does one oscillation lag behind the other, to what extent, and is this consistent across cells?

\subsection{Cross-correlation}
\label{sec:analysis-correlation-xcf}

Cross-correlation, widely used in signal processing, is used to measure how similar two time series are as the displacement of one relative to the other is shifted across the length of the time series.
To put it in the context of autocorrelation discussed earlier, autocorrelation is a special case of cross-correlation in which the two signals are identical.

In a biological context, cross-correlation has been used to investigate how fluctuations in transcription levels influence fluctuations in protein expression in \textit{E. coli} [CHECK CITATION AGAIN TO VERIFY IF TRUE] \parencite{dunlopRegulatoryActivityRevealed2008},
and to investigate the relationship between instantaneous growth rate and the expression of \textit{lac} genes OR of enzymes in central metabolism across a population of \textit{E. coli} cells \parencite{kivietStochasticityMetabolismGrowth2014}.
Importantly, these studies are derived from single-cell microfluidics experiments with time-lapse microscopy, and therefore their treatment of a population of time series is useful for my case.

Therefore, my use of cross-correlation is adapted from theirs so that it suits a population of time series, which is the thing that I have.
My implementation has already been described in section~\ref{subsubsec:analysis-characterisation-acf-maths}.

As I did for my discussion of autocorrelation, I will start with the sinusoid and FitzHugh-Nagumo oscillators as simulations of my biological time series to show understanding of the methods,
then move on to an example of a real dataset.

% - Cross-correlation: start from the basic definitions, then extend to population-level cross-correlation as used by Kiviet et al. (2014)

% Copied from project: 'Modelling cross-correlation between sinusoids and relaxation oscillators',
% or, my 'synthetic oscillations report'.
% Currently doesn't really fit the thesis so much -- it comes with its own subsections, and this part can definitely be shorter.  Certainly, there shouldn't be a 'results' subsection.

% (some text used to be here)

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
  % TODO: Squish so that it is consistent with adjacent figure
    \includegraphics[width=\linewidth]{sinusoid_and_fitzhughnagumo_nonoise.png}
    \caption{
      Sample FitzHugh-Nagumo oscillator (orange) generated with parameters $RI_{\mathrm{ext}}$ = 0.4, $\tau$ = 12.5, $a$ = 0.7, $b$ = 0.82, and a sinusoid of frequency 0.026 (blue), adjusted so that its frequency is the same as that of the FitzHugh-Nagumo oscillator.
    }
    \label{fig:xcf-nonoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{randomshift_sinusoid_fitzhughnagumo_xcf.png}
    \caption{
      Cross-correlation function of the FitzHugh-Nagumo oscillator with respect to the sinusoid oscillator, based on 400 pairs of signal as in~\ref{fig:xcf-nonoise-ts}, each pair randomly phase-shifted.
    }
    \label{fig:xcf-nonoise-xcf}
  \end{subfigure}

  % TODO: Use Gillespie noise.  Maybe this figure already exists.
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{sinusoid_and_fitzhughnagumo.png}
    \caption{
      FitzHugh-Nagumo (orange) and sinusoid oscillators (blue), as in~\ref{fig:xcf-nonoise-ts}, but with Gaussian noise of standard deviation 0.03 added.
    }
    \label{fig:xcf-gausnoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{randomshift_sinusoid_fitzhughnagumo_noisy_xcf.png}
    \caption{
      Cross-correlation function of the FitzHugh-Nagumo oscillator with respect to the sinusoid oscillator, based on 400 pairs of signal as in~\ref{fig:xcf-gausnoise-ts}, each pair randomly phase-shifted.
    }
    \label{fig:xcf-gausnoise-xcf}
  \end{subfigure}

  \caption{
    Using the cross-correlation function to evaluate the shift of one synthetic time series relative to another with a different shape.
  }
  \label{fig:xcf}
\end{figure}

Shift of this function from the origin indicates the lag of one time series with respect to another.  This is the point of using cross-correlation, i.e. quantifying this lag across a population of time series.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.9\textwidth}
  \centering
    \includegraphics[width=\linewidth]{htb2mCherry_26643_plots_purple_01.pdf}
    \caption{
      Sample time series of flavin autofluorescence (purple) and histone 2B abundance (pink).
    }
    \label{fig:xcf-biol-ts}
  \end{subfigure}

  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{xcf.pdf}
    \caption{
      Cross-correlation function of the histone 2B abundance time series with respect to the flavin autofluorescence time series.
    }
    \label{fig:xcf-biol}
  \end{subfigure}

  \caption{
    Using the cross-correlation function to evaluate the shift of histone 2B abundance traces with respect to flavin autofluorescence traces.
  }
  \label{fig:xcf}
\end{figure}

Can use this method to quantify the shift of the cell division cycle relative to the yeast metabolic cycle, therefore assessing the relationship between the two oscillators.

% \subsection{Granger Causality}
% \label{sec:analysis-correlation-granger}

% Granger causality is another method of assessing the relationship between two time series.
% It is a statistical hypothesis test used to answer the question of whether one time series is useful for predicting another [CITATION FOR GRANGER CAUSALITY TEST NEEDED] -- in other words, it evaluates precedence of one time series relative to another.
% The logic of the Granger causality test is that: given a time series X and a time series Y, if you can predict values of Y based on the values of X better than you can predict values of Y based on the past values of Y, then X Granger-causes Y.
% This method is used a lot in econometrics and climate science [CITATIONS NEEDED].
% [AND DO BIOLOGISTS USE IT?]

% (Add content when I figure out how to make this investigation work.)

\section[Clustering]{Clustering: I have many time series (of the same signal) from many cells -- what are their relationships to each other?}
\label{sec:analysis-clustering}

% Literature review subsection
% \subsection{(Literature)}
% \label{subsec:analysis-clustering-literature}

% mini-review -- IT'S PRETTY UNWIELDY AT THIS POINT, AND REFERENCES CONCEPTS NOT DISCUSSED TILL LATER
Clustering of time series has been used to identify groupings within a set of time series \parencite{wangStructureBasedStatisticalFeatures2007}, including biological time series \parencite{shafieiDopamineSignalingModulates2019}, or even transcript cycling patterns in YMCs \parencite{tuLogicYeastMetabolic2005}.
However, \textcite{wangStructureBasedStatisticalFeatures2007} and \textcite{tuLogicYeastMetabolic2005} employed $k$-means clustering, which requires the user to specify the number of clusters, and therefore may not reflect the underlying structure of the set of time series.
Though \textcite{shafieiDopamineSignalingModulates2019} used modularity clustering \parencite{newmanModularityCommunityStructure2006}, the method was based on one time series feature, which may not adequately capture the characteristics of the time series.

For my investigation of the yeast metabolic cycle, clustering is important because it can discover structure in a dataset and can summarise the difference between the groups it finds.
And I can also ask the question of whether such differences conform to biological groupings in the dataset -- if so, I can then ask the question of which time series features from each biological grouping is responsible for this.
Furthermore, clustering is important for a dataset from which we expect cell-to-cell heterogeneity.
This thus takes advantage of single-cell microfluidics, as such a platform allows us to study such heterogeneity.

% \subsection{Machine learning approaches to clustering}
% \label{subsec:analysis-clustering-ml}
% - Featurisation -- decisions to make
% - Clustering approaches and algorithms -- compare and contrast
% - Review existing methods first and then talk about the methods I tried, with results.

% Copied from 10m report.
% - Supervised classification is going to be mostly killed because it belongs in a previous section, and I have better data than SM vs SC.
% - Unsupervised clustering: the literature stays, but I'll try doing it on better data.  To add: UMAP.
% ----------------------
\subsection{Graph-based clustering of oscillations based on feature vectors}
\label{subsec:analysis-clustering-graphclustering}

Thus, I converted each time series of flavin autofluorescence to a vector of features, and represented the vectors as nodes on a geometric graph.
Such a geometric graph may function as a visualisation tool as well.

% FIGURE: pipeline and options.  It's rather difficult to visualise what happens, so such a figure, like the one I often use in the lab presentations in the summer, will be helpful.  Though may be more impactful in the presentation rather than in this report.

Modularity clustering is the problem of partitioning a graph into clusters in order to optimise a `modularity' value.
This value is defined as the number of edges between clusters minus the number of expected edges if edges are placed at random \parencite{newmanModularityCommunityStructure2006}.
The general Louvain algorithm \parencite{blondelFastUnfoldingCommunities2008,muchaCommunityStructureTimeDependent2010} is one method to solve this optimisation problem and define partitions for a graph.
This algorithm has a resolution parameter which specifies the scale of the clusters: low resolution gives large clusters, and high resolution gives small clusters \parencite{fortunatoResolutionLimitCommunity2007}.

Unsupervised graph-based clustering suggested that the division between complete and minimal media was dominant among time series of flavin autofluorescence. %(figure \ref{fig:EffectofMediaClusters}).
I constructed an incomplete graph to represent the pairwise similarity between time series based on the \textit{catch22} feature vectors.
Then, I identified communities in the resulting network using the general Louvain algorithm.
When the resolution value was set so that the graph was partitioned into two clusters, the clusters conformed well to complete and minimal media groups (figure \ref{fig:EffectofMediaClusters}). % TABLE/NUMBER: Add a 'confusion matrix' (re-use code from 5m) and calculate accuracy (note: revert to some old commit to re-produce this -- why the hell did Fulcher have to change EVERYTHING ughhhhh).  Make clear the point that in each iteration, things are different.
% (this is the where the Rand value can be used instead, but there is no need -- just making a note here).
Such results suggested that unsupervised graph-based clustering may have potential to discover structure in the dataset.

% TODO: REPEAT INVESTIGATION WITH BETTER DATA

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{10m_EffectofMediaClusters}
  \caption{Graph-based clustering partitions a geometric graph defined by pairwise cosine distances between \textit{catch22} vectors close to labels defined by media.}
  \label{fig:EffectofMediaClusters}
\end{figure}

\subsection{UMAP Based on Feature Vectors}
\label{subsec:analysis-clustering-umap}

% TODO: repeat with better datasets
% And it probably saves more time to go over the code again, draft new figures that show my point, and generate new figures, rather than trying to massage the existing figures into this structure.
UMAP \parencite{mcinnesUMAPUniformManifold2020} is a unsupervised dimension reduction technique that can be used to visualise data, and it preserves the distances between points.
% re-phrase this further to prevent plagiarism
Specifically, UMAP aims to find a manifold structure of the input observations and compute a low-dimensional embedding that preserves the topological structure of the manifold.
This embedding thus serve as coordinates to plot the data onto a low-dimension space so as to preserve any clustering of the data.

UMAP has several caveats.
It lacks strong interpretability, namely: dimensions do not mean anything, unlike in PCA.
Additionally, it assumes that the data has a manifold structure.
So it tends to find manifold structure within the noise of a dataset, like how humans find constellations among stars.
If more data are sampled, then the amount of structure from noise will tend to decrease.
It assumes that local distance is more important than long-range distances, like t-SNE.
And finally, approximations were made to improve computational efficiency.

UMAP has several hyperparameters, of which four have major effects on the embedding:
\begin{itemize}
  \item The \emph{number of neighbours} to consider when approximating the local metric controls how the method balances local and global structure in the data.
        With low values of this parameter, the algorithm concentrates on very local structure, potentially to the detriment of the big picture.
        As the value increases, the algorithm `glues' more nodes together to form clusters.
  \item The \emph{largest embedding dimension} controls the number of dimensions the data is reduced to.
        In other words, it controls whether the resulting map is one-dimensional, two-dimensional, three-dimensional, or of higher dimensions.
  \item The \emph{minimal distance} controls the desired separation between close points in the embedding space.
        Specifically, this parameter controls how tightly the algorithm is allowed to pack points together.
        With low values, the visualisation forms `clumps'.
  \item The previous hyperparameters are numerical, but the \emph{metric} hyperparameter instead specifies the distance metric that is used to compute distances in the ambient space of the input data.
        For example, this metric can be the Euclidean distance, the cosine distance, or other metrics used to compute the distances between two vectors of numerical data.
\end{itemize}

I used UMAP to see if there were intrinsic structures in my datasets, and whether these structures found by such as unsupervised method corresponded to labels, whether oscillatory vs non-oscillatory or labels that distinguish cells from different strains or nutrient conditions.

I used \textit{catch22} as features

% Copied from org note: UMAP to discover structures in flavin datasets

% Notes
% 'labelling custom' refers to using manually-determined oscillatory (0) vs non-oscillatory (1) categories as labels

[FIGURES: 'figures 13-16' from \_resources/umap-figures]

Observations
\begin{itemize}
\item Using labels (strains, oscillation categories) to supervise UMAP works very well.
\item Using catch22 as features doesn't seem to do much better than using raw time points as features.
\item The most interesting plots are figures 13-16:
\begin{itemize}
\item Figure 15 shows that there are two clear clusters of non-oscillatory time series.  This suggests that unsupervised UMAP can discriminate some non-oscillatory time series from the rest.  It's possible that the non-oscillatory time series that cluster with the oscillatory ones may be 'borderline'.  Additionally, comparing this figure with figure 11 shows that using catch22 as features is superior to using time points as features in this context.  The reason these clusters exist in figure 15 and not figure 7 (the equivalent in the other dataset) may be because the BY4741-ZWF1 dataset contains many very high-quality oscillations that are clearly different from the non-oscillatory time series.
\item Comparing figure 15 with figure 13 shows that these non-oscillatory time series are mostly zwf1$\Delta$.  It makes sense because this set of cells have a large group of non-oscillatory flavin signals.
\item Figure 14 shows that there are three clusters among the zwf1$\Delta$ cells, despite supervised UMAP.  Perhaps these have different shapes.  Similarly, figure 16 shows three clusters of non-oscillatory time series; perhaps there are three main non-oscillatory shapes.
\end{itemize}
\end{itemize}

Discussion
\begin{itemize}
\item May need to change the hyperparameters to see if different strains appear in different groups, but it seems to be doing well for some of the plots.
\item Definitely need to visualise the time series associated with each node in the figures; this might reveal the logic behind the structures.  bokeh can be used (see \url{https://umap-learn.readthedocs.io/en/latest/basic\_usage.html}).
\end{itemize}

(Space for discussion according to the figure below.  For example, what can I conclude from this?)

[FIGURE: GRID-SEARCH UMAP PARAMETERS.  THIS FIGURE SHOULD HAVE A 2D GRID OF UMAP PLOTS AS TWO OF THE PARAMETERS ARE VARIED.]

\section{Summary}
\label{sec:analysis-summary}
% - Justification of each steps of my pipeline

% I expect this will be improved when I go through the whole analysis again and get good take-home messages in my head.
I've described the way that I process my single-cell time series data, broken down into five main steps: cleaning data, classification of oscillatory time series, characterisation of features of oscillatory time series, clustering of similar time series in a large dataset, and correlation of two related time series.
This can form an analysis workflow which can be adapted to other biological rhythms or analysis of large datasets of time series in general.
And this can be an interesting software engineering question to bring it all together.

In each step of the way, there are several methods to choose from, each with its own merits and caveats -- mostly because there doesn't seem to be an agreed-upon standardised way to solve problems.
This is particularly highlighted in the many `judgement calls'.
In cleaning data, the analyst must decide on a threshold to filter out useless data from the useful ones, but without compromising on the size of the resulting dataset so the remaining dataset is still useful.
In classification of oscillatory time series: rhythmicity detection inevitably hinges upon defining a threshold between `oscillatory' and `non-oscillatory'.
Furthermore, it is difficult to do this with noisy and relatively short (on the order of 10 oscillations, rather than 1000) time series.
Characterisation of oscillatory time series is more `objective', but there are issues in the representation of properties like shape and phase.
And the machine learning methods rely on supplying a training set, choosing appropriate featurisation methods, and tuning hyperparameters, as with any other machine learning method.

Solving mathematical and computational problems for each task is an intellectual investigation in its own right (each can spin off to form its own thesis), and opens more questions than answers.
And so future directions include things like using Bayesian method to classify oscillations, or whether better data or better featurisation helps.

I illustrate the use of my analysis workflow in the next chapter, when I apply it to biological results.
