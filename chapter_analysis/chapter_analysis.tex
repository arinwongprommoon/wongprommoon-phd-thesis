% Much of these are still open because I haven't leveraged more recent (i.e. better) data.  So worth re-doing the analysis with it (I have better data & I'm better at coding & after writing this I see how it all fits together).

% Does it make sense to introduce methods (plus investigations, specifically for ACF/XCF) BEFORE describing the whole 'pipeline'?  This chapter is getting unwieldy, especially when I go back and forth between the mathematical methods.
\chapter{Analysis of oscillatory time series in the yeast metabolic cycle}
\label{ch:analysis}

To characterise the properties of large sets of time series generated by microfluidics experiments, I sought to develop a pipeline of time series analysis methods.

Short and noisy oscillatory time series are challenging to analyse to give usable characteristics such as period and phase, especially because of the limited information they encode.
To illustrate the challenge, microfluidics experiments capture up to 5--10 periods metabolic cycles, so Fourier analysis gives period estimates at low resolution.
In addition, there is no standard method of analysing oscillatory time series such as those that arise from the yeast metabolic cycle.

Here, I propose steps for the analysis of datasets of 100--1000 time series related to the yeast metabolic cycle from single cells, using my experimental data as an example case.

Specifically, I address:
\begin{enumerate}
  \item Data cleaning: choosing data and filtering out long-term trends that may confound further analysis.
  \item Visualising groups in a dataset: identifying the relationship between multiple time series of the same type of signal from a population of cells.
        Such a relationship may include groupings or structures within the population of time series.
  \item Detection of rhythmicity: determining whether a time series exhibits oscillations.
  \item Period estimation: identifying the period of a single time series.
        In this section, I discuss the use of the autocorrelation function.
  \item Detection of synchrony: identifying the relationship between two types of signal from the same cell.
\end{enumerate}

In this chapter, I will ... (REVISIT THIS AFTER I GO THROUGH EVERYTHING)


\section{Analysing time series in a biological context}
\label{sec:analysis-literature}

Previous studies have described mathematical and computational methods to analyse biological time series.

\textcite{zielinskiPeriodEstimationRhythm2022} described a software pipeline (BioDare/BioDare2) to estimate the period and detect rhythms in time series catered to circadian rhythm studies.
This pipeline includes choices of detrending and normalising data, followed by a choice of methods to estimate the period, phase, and amplitude of the time series.
Furthermore, the pipeline also includes an implementation of the JTK\_CYCLE test \parencite{hughesJTK_CYCLEEfficientNonparametric2010} along with an empirical derivative \parencite{hutchisonImprovedStatisticalMethods2015a} as statistical tests for the presence of an oscillation in a time series.

BioDare2 builds upon \textcite{zielinskiStrengthsLimitationsPeriod2014}, which compared and contrasted a set of period-estimation methods ---
% shopping-list?
% Also: define these/add citations
FFT-NLLS, mFourFit, MESA, the Enright periodogram, the Lomb-Scargle periodogram, and spectrum resampling --- to conclude with recommendations on time series analysis.
These recommendations include:
\begin{enumerate}
  \item \emph{Amount of data:} To determine whether a time series is oscillatory, at least 2.5 cycles are needed.
        To estimate the period to within \SI{0.5}{\hour} for a \SI{24}{\hour} period, at least 5 cycles are needed.
  \item \emph{Data pre-processing:} Baseline trends must be removed.
  \item \emph{Detecting rhythmicity:} The Enright periodogram can be used.
  \item \emph{Estimating period:} mFourFit and MESA should be used as a minimum and agreement between the two methods is a good indicator of accuracy because the methods are based on different principles.
        Additionally, FFT-NLLS can be used to provide error measures for the period, phase, and amplitude.
\end{enumerate}

Studies of biological rhythms have used the BioDare pipeline to quantify features of oscillations of fluorescence.
These include using FFT-NLLS to calculate the period and amplitude error of fluorescence in mouse brain sections to determine the mechanistic basis of the synchronisation of the suprachiasmatic nucleus \parencite{hamnettVasoactiveIntestinalPeptide2019}, and using linear detrending of data followed by FFT-NLLS and spectral resampling to estimate the period and amplitude error of delayed fluorescence of chloroplasts in \textit{Kalancho\"{e} fedtschenkoi} leaves to determine whether phosphorylation of phosphoenolpyruvate affects robustness of circadian rhythms \parencite{boxallPhosphorylationPhosphoenolpyruvateCarboxylase2017}.

As another example, \textcite{fulcherHctsaComputationalFramework2017} described a software pipeline, termed \textit{hctsa}, that computes over 7700 time series features for input time series.
The resulting feature matrix --- a row for each time series and a column for each feature --- could then be used to identify sets of features that are useful to discriminate between sets of time series that corresponding to biological groups, or to identify clusters of time series based on their properties.
The publication then showed that \textit{hctsa} could be used to distinguish five \textit{Caenorhabditis elegans} strains based on their movement patterns, and to identify clusters in the feature space of time series of \textit{Drosophila melanogaster} movement patterns which correspond well to experimental groups.
To reduce computation time, \textcite{lubbaCatch22CAnonicalTimeseries2019} identified 22 features, termed \textit{catch22}, of \textit{hctsa} that performed well in time series classification tasks based on 93 datasets.

Taken together, the two examples of BioDare and \textit{hctsa} demonstrate two approaches to analysis of time series: on one end, relying on mathematical methods, and on the other end, a data science approach to time series classification tasks.


\section{Data cleaning: filtering out long-term trends}
\label{sec:analysis-cleaning}

Biological time series often have long-term trends that are not useful and may confound analysis.
In my study of the yeast metabolic cycle, such trends include slow, global changes in flavin autofluorescence, which must be removed to uncover the periodic behaviour of flavin autofluorescence that is a component of the metabolic cycle.
To determine the detrending method that is most appropriate for my data, I compared a frequency filtering method, a sliding-window detrending method, and a polynomial detrending method.

\textcite{zielinskiPeriodEstimationRhythm2022} describes several methods of detrending time series:
% Clarify definitions -- this is currently like a shopping list.
\begin{enumerate}
  \item Polynomial detrending (including cubic linear detrending)
  \item Sliding-window detrending (termed as baseline detrending)
  \item Amplitude-and-baseline detrending (modifies sliding-window detrending by remove amplitude dampening)
\end{enumerate}

However, each method has its own caveats.
Polynomial detrending assumes a polynomial-shaped underlying signal, while methods based an sliding-window detrending necessitates discarding time points at the beginning and end of the time series and may introduce strong artefacts.

\begin{figure}
  \centering
  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_raw}
   \caption{
   }
   \label{fig:analysis-filter-raw}
  \end{subfigure}

  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_butterworth}
   \caption{
   }
   \label{fig:analysis-filter-butterworth}
  \end{subfigure}

  \begin{subfigure}[htpb]{0.8\textwidth}
   \centering
   \includegraphics[width=\textwidth]{fft_slidingwindow}
   \caption{
   }
   \label{fig:analysis-filter-movavg}
  \end{subfigure}

  \caption{
    (Left panels) Time series and (right panels) Fourier spectra corresponding to
    \textbf{(\ref{fig:analysis-filter-raw})}
    a sample raw time series of flavin autofluorescence,
    \textbf{(\ref{fig:analysis-filter-butterworth})}
    the time series processed by a high-pass Butterworth filter with a critical frequency \SI[parse-numbers=false]{1/350}{\minute^{-1}}, and
    \textbf{(\ref{fig:analysis-filter-movavg})}
    the time series detrended using a moving average (window size 30).
  }
  \label{fig:analysis-filter}
\end{figure}

To demonstrate the use of a method that modifies the frequency profile of time series to remove trends, Figs.\ \ref{fig:analysis-filter-raw}--\ref{fig:analysis-filter-butterworth} show how a time series and its Fourier spectrum changes after the application of a high-pass Butterworth filter with a critical frequency of \SI[parse-numbers=false]{1/350}{\minute^{-1}}.
Unlike sliding-window methods, defining a signal filter offers direct control over frequencies.
The critical frequency in this case was defined to create a reasonable upper limit of periods of the yeast metabolic cycle, based on my observations in single-cell microfluidics experiments.
Defining this critical frequency is a trade-off between emphasising metabolic oscillations in expected frequency windows and excluding the possibility of long-period metabolic cycles.

To show how sliding-window methods adversely affect the frequency profile of time series when used for detrending, I computed the Fourier spectrum of time series detrended using the moving average method.% and the Savitzky-Golay filter.
Sliding-window methods are common in detrending biological time series.
For example, \textcite{cunyHighresolutionMassMeasurements2022} used a moving average: a constant, defined sliding window to smooth time series of yeast cell mass during growth.
\textcite{papagiannakisAutonomousMetabolicOscillations2017} fitted a smoothing spline to their single-cell fluorescence data of the yeast metabolic cycle, then divided the data points by the smoothing spline to detrend their data; the specific mathematical method used to define the smoothing spline was unclear in their study.
% Equations?
%The Savitzky-Golay filter is also common --- this method is based on fitting a sliding window of a defined number of time points with a polynomial of a defined order using least squares fitting.

Fig.\ \ref{fig:analysis-filter-movavg} shows that the moving average method introduced an artefact in the frequency spectrum near the reciprocal of the window size and decreases the number of time points.
%In contrast, Fig.\ \ref{fig:analysis-slidingwindow-savgol} shows that the Savitzky-Golay filter ...(INSERT DISCUSSION HERE)...
In sum, sliding-window methods were not ideal because they distorted the data in a way that affects conclusions, or removed noise information that may be useful.
% Really?
%Importantly, it is not possible to restore the raw data when it is processed using such methods.

Therefore, for subsequent analyses, I used the high-pass Butterworth filter defined above to process raw fluorescence time series.
This is because using this method, I obtain more control over the frequency components, and the methods preserves noise frequencies that are useful for assessing the quality of the data set and estimating the noise properties.


%\section[Clustering]{Clustering: I have many time series (of the same signal) from many cells --- what are their relationships to each other?}
\section{Visualising groups in the dataset}
\label{sec:analysis-clustering}

To identify structures or natural groupings in datasets of potentially oscillatory time series, I implemented dimension reduction and graph-based clustering methods; specifically, UMAP and modularity clustering.
Such data visualisation methods are important because the structures they show may identify differences between groups that are biologically relevant --- for example, subpopulations of oscillations with similar properties.
Previous efforts in using computational methods to identify groups in a set of biological time series include using $k$-means clustering to identify clusters of transcript cycling patterns that correspond to phases of the YMC \parencite{tuLogicYeastMetabolic2005}, development of a method to cluster featurised multivariate time series based of videos of human motion \parencite{wangStructureBasedStatisticalFeatures2007}, and using signal entropy to featurise fMRI signals followed by modularity clustering to partition the signals into brain regions.

% I don't like this sentence
To demonstrate the data visualisation methods, I used time series of flavin autofluorescence oscillation from one experiment with both BY4741 (wild-type strain) and \textit{zwf1$\Delta$} (mutant strain) strains.
%I expected the \textit{zwf1$\Delta$} traces to show different properties, as it is a strain that did not show oscillations in dissolved oxygen in the chemostat~\parencite{tuCyclicChangesMetabolic2007}.


\subsection{UMAP}
\label{subsec:analysis-clustering-umap}

% TODO: repeat with better datasets
% And it probably saves more time to go over the code again, draft new figures that show my point, and generate new figures, rather than trying to massage the existing figures into this structure.
UMAP \parencite{mcinnesUMAPUniformManifold2020} is a unsupervised dimension reduction method that can be used to visualise structure in a dataset, and it preserves the distances between points.
% EQUATIONS?
Specifically, UMAP aims to find a manifold structure of the input observations and compute a low-dimensional embedding that preserves the topological structure of the manifold.
This embedding thus serve as coordinates to plot the data onto a low-dimensional space so as to preserve any clustering in the data.

UMAP has several hyperparameters, of which four have major effects on the embedding:
\begin{itemize}
  \item The \emph{number of neighbours} ($n$) to consider when approximating the local metric controls how the method balances local and global structure in the data.
        With low values of this parameter, the algorithm concentrates on very local structure, potentially to the detriment of the big picture.
        As the value increases, the algorithm `glues' more nodes together to form clusters.
  \item The \emph{largest embedding dimension} ($d$) controls the number of dimensions the data is reduced to.
        In other words, it controls whether the resulting map is one-dimensional, two-dimensional, three-dimensional, or of higher dimensions.
  \item The \emph{minimal distance} ($\mathrm{min\_dist}$) controls the desired separation between close points in the embedding space.
        Specifically, this parameter controls how tightly the algorithm is allowed to pack points together.
        With low values, the visualisation forms `clumps'.
  \item The previous hyperparameters are numerical, but the \emph{metric} hyperparameter instead specifies the distance metric that is used to compute distances in the ambient space of the input data.
        For example, this metric can be the Euclidean distance, the cosine distance, or other metrics used to compute the distances between two vectors of numerical data.
\end{itemize}

% MOVE TO DISCUSSION?
% However, UMAP has several caveats.
% It lacks strong interpretability, namely: dimensions do not mean anything, unlike in principal component analysis (PCA).
% Additionally, it assumes that the data has a manifold structure.
% So it tends to find manifold structure within the noise of a dataset, like how humans find constellations among stars.
% If more data are sampled, then the amount of structure from noise will tend to decrease.
% It assumes that local distance is more important than long-range distances, like t-SNE.
% And finally, approximations were made to improve computational efficiency.


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
  \centering
    \includegraphics[width=\linewidth]{umap_single_is20016_2_edit.png}
    \caption{
    }
    \label{fig:umap-osc}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
  \centering
    \includegraphics[width=\linewidth]{umap_single_is20016_1_edit.png}
    \caption{
    }
    \label{fig:umap-strain}
  \end{subfigure}

  \caption[
      UMAP embedding of a dataset of time series featurised using \textit{catch22}.
    ]{
      UMAP embedding ($n=5$, $\mathrm{min\_dist} = 0.5$, $d=2$, Euclidean distance as the metric) of a dataset of time series featurised using \textit{catch22}.
      Each node represents a time series, coloured either by
      \textbf{(\ref{fig:umap-osc})}
      whether each is oscillatory or not, human-labelled, or by
      \textbf{(\ref{fig:umap-strain})}
      strain (`BY4741' or `\textit{zwf1$\Delta$}').
    }
  \label{fig:umap}
\end{figure}

To assess the performance of UMAP for time series data, I evaluated whether the method was able to discover a structure within the BY4741 \& \textit{zwf1$\Delta$} dataset that corresponded to the division between the two strains or between oscillatory and non-oscillatory time series.
Fig.\ \ref{fig:umap-osc} demonstrates that UMAP suggests a small group of non-oscillatory time series that differed markedly from the rest, and a larger group that showed more similarity with oscillatory time series.
In addition, Fig.\ \ref{fig:umap-strain} demonstrates that UMAP suggests that the BY4741 time series were more similar to each other in contrast to \textit{zwf1$\Delta$} occupying more positions within the space; this agreed with the observation that time series from this strain had a larger variety of shapes and oscillation quality.
Thus, UMAP may have potential to discover structure in a dataset, especially when separating oscillatory and non-oscillatory time series, or when separating time series of different shapes.
% Need: some proof, e.g. representative time series of each cluster -- but maybe it is not possible because UMAP isn't strictly a clustering method, but more of a visualisation method.  Perhaps this is more appropriate to the graph clustering part.
% However, some non-oscillatory time series clustered with oscillatory time series, possibly because these time series were borderline between the two categories.
% In addition, ...(POTENTIALLY ADD SOME DISCUSSION ABOUT THE STRAINS THAT I HAVE COMMENTED OUT)...

% The reason these clusters exist in this figure is likely because the BY4741 \& \textit{zwf1$\Delta$} dataset contains many very high-quality oscillations that are clearly different from the non-oscillatory time series.
% Comparing figure~\ref{fig:umap-osc} with figure~\ref{fig:umap-strain} shows that these non-oscillatory time series are mostly \textit{zwf1$\Delta$}.
%It makes sense because this set of cells have a large group of non-oscillatory flavin signals.

\begin{figure}
  \centering
    \includegraphics[width=0.9\linewidth]{umap_grid_is20016_edit.png}
    \caption[
      Grid search of UMAP hyperparameters
    ]{
      Grid search of UMAP hyperparameters: number of neighbours along the horizontal axis and minimum distance along the vertical axis.
      Data points are coloured according to category: grey indicates non-oscillatory time series, orange indicates oscillatory time series from BY4741 cells, and blue indicates oscillatory time series from \textit{zwf1$\Delta$} cells.
    }
  \label{fig:umap-gridsearch}
\end{figure}

To improve the visualisation, I performed a grid search of the $n$ and $\mathrm{min\_dist}$ hyperparameters to find the best combination.
Fig.\ \ref{fig:umap-gridsearch} suggests that $50 \leq n \leq 150$ and $0.25 \leq \mathrm{min\_dist} \leq 1$ resulted in a good separation between the the BY4741 and \textit{zwf1$\Delta$} nodes.
In addition, non-oscillatory time series consistently organised into groups distinct from the rest as the hyperparameters were varied.


\subsection{Graph-based clustering}
\label{subsec:analysis-clustering-graphclustering}

Modularity clustering is a mathematical method to partition a graph into clusters in order to optimise a `modularity' value, defined so that the method finds a trade-off between maximising the connections within a cluster and minimising the connections between clusters \parencite{newmanModularityCommunityStructure2006}.
% [ADD EQUATIONS]
%This value is defined as the number of edges between clusters minus the number of expected edges if edges are placed at random \parencite{newmanModularityCommunityStructure2006}.
This optimisation problem is computationally difficult, so approximations such as the Louvain algorithm are needed for large networks \parencite{blondelFastUnfoldingCommunities2008}.
Furthermore, the intrinsic scale of modularity scales with the square root of the number of connections in the network; therefore, if the network is large, there is a large resolution limit, preventing a modularity clustering algorithm from detecting small-scale structures \parencite{fortunatoResolutionLimitCommunity2007,traagNarrowScopeResolutionlimitfree2011}.
To remedy this, algorithms that implement a resolution parameter ($\gamma$), were devised; the value of this parameter thus controls the scale at which communities are detected \parencite{reichardtDetectingFuzzyCommunity2004,kumpulaLimitedResolutionComplex2007}.
In addition, the Leiden algorithm \parencite{traagLouvainLeidenGuaranteeing2019} was developed to ensure that communities are well-connected, based on the earlier Louvain algorithm, and provides an optimum number of communities.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
  \centering
    \includegraphics[width=\linewidth]{graph_representation}
    \caption{
    }
    \label{fig:analysis-clustering-modclust-graph}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{pruning}
    \caption{
    }
    \label{fig:analysis-clustering-modclust-prune}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{newmanModularityCommunityStructure2006_1}
    \caption{
    }
    \label{fig:analysis-clustering-modclust-modclust}
  \end{subfigure}

  \caption[
    Modularity clustering visualises natural groupings of time series
  ]{
    Process of preparing a dataset of time series for modularity clustering.
    \textbf{(\ref{fig:analysis-clustering-modclust-graph})}
    Constructing a graph representation,
    \textbf{(\ref{fig:analysis-clustering-modclust-prune})}
    pruning the complete graph, and
    \textbf{(\ref{fig:analysis-clustering-modclust-modclust})}
    modularity clustering.
    \ref{fig:analysis-clustering-modclust-modclust} adapted from \textcite{newmanModularityCommunityStructure2006}.
  }
  \label{fig:analysis-clustering-modclust}
\end{figure}

To assess the performance of a graph-based clustering method in identify clusters in time series data, I represented a dataset of time series as a graph before using modularity clustering to identify clusters.
Fig.\ \ref{fig:analysis-clustering-modclust} illustrates this process, specifically:
\begin{enumerate}
  \item \emph{Constructing a graph representation:}
        Each time series was represented as a vector of features in $n$-dimensional space, where $n$ is the length of the vector.
        Here, I represented each time series with a vector of 22 features using \textit{catch22}.
        The cosine distances between each pair of vector was computed, and became the edge weights of a complete graph with each time series as a node.
  \item \emph{Pruning:}
        The complete graph was pruned by deleting edges, so that each node had at least $k$ neighbours.
        Here, $k=10$.
  \item \emph{Modularity clustering:}
        Modularity clustering was performed on the graph to partition the pruned graph into communities.
        In this step, I used the Leiden algorithm \parencite{traagLouvainLeidenGuaranteeing2019}.
\end{enumerate}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{graphclust_combined_is20016.pdf}
    \caption{
    }
    \label{fig:graphclustering-combined}
  \end{subfigure}

  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{graphclust_leiden_is20016.pdf}
    \caption{
    }
    \label{fig:graphclustering-leiden}
  \end{subfigure}

  \caption{
    Pruned geometric graph of BY4741 and \text{zwf1$\Delta$} time series from the same experiment.
    \textbf{(\ref{fig:graphclustering-combined})}
    Nodes coloured by group: grey, non-oscillatory; blue, oscillatory \textit{zwf1$\Delta$}; orange, oscillatory BY4741.
    Thickness of edges represent edge weights, scaled by similarity found by cosine distances.
    \textbf{(\ref{fig:graphclustering-leiden})}
    Nodes coloured by community (out of 10) as optimised by the Leiden algorithm.
    Edges within a community are in black, while edges between communities are in light grey.
  }
  \label{fig:graphclustering}
\end{figure}

Fig.\ \ref{fig:graphclustering-combined} shows that construction of a pruned graph based on similarities between time series highlights two groups of non-oscillatory time series, similar to the UMAP representation (Fig.\ \ref{fig:umap}).
Furthermore, Fig.\ \ref{fig:graphclustering-leiden} suggests that these two groups belong to separate communities, and shows that modularity clustering was able to further show communities among the oscillatory time series.
However, such communities did not divide cleanly along the division between BY4741 and \textit{zwf$\Delta$} cells, suggesting that time series features alone are not able to divide these two strains.
This agreed with my observation that some oscillatory \text{zwf$\Delta$} time series resembled BY4741 time series.

% PLOT AVERAGE TIME SERIES FROM EACH COMMUNITY?

In sum, the general agreement between UMAP and modularity clustering shows that the BY4741-\textit{zwf$\Delta$} dataset had internal structure defined by rhythmicity of time series.


%\section[Classification]{Classification: is my time series oscillatory?}
\section{Detection of rhythmicity}
\label{sec:analysis-classification}

To assess how a perturbation affects the YMC, it is important to have a systematic method to determine whether a time series is oscillatory.
Determining whether a time series is oscillatory, or rhythmicity detection, is challenging because the task of defining the presence of an oscillation breaks down when the frequency domain is considered.
Rhythmicity detection is best re-defined as identifying the presence of a frequency in a range of interest, excluding lower frequencies that contribute to trends, and higher frequencies that contribute to noise \parencite{zielinskiStrengthsLimitationsPeriod2014}.
However, there is no way to objectively specify a failure rate for a rhythmicity detection method as there is no independent method to estimate rhythmicity; therefore, evaluating such a classification method requires a subjective definition of whether each time series is oscillatory.

To determine the time series classification method that is most appropriate for my data, I compared a spectral method, a model-fitting method, and a machine learning method.


\subsection{Rhythmicity detection using spectral methods}
\label{subsec:analysis-classification-spectral}

In order to classify oscillatory and non-oscillatory time series, I modified a classifier based on a spectral method that included a statistical test (section~\ref{subsec:methods-computational-periodogram}).
This classifier was based on \textcite{glynnDetectingPeriodicPatterns2006a}, which described a method that employed the peak power from the Lomb-Scargle periodogram \parencite{lombLeastsquaresFrequencyAnalysis1976} to rank time series by the quality of oscillation and to perform a statistical test to determine whether a time series is oscillatory or non-oscillatory \parencite{scargleStudiesAstronomicalTime1982}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.65\textwidth}
  \centering
    \includegraphics[width=\linewidth]{glynn_is20016_1_edit.pdf}
    \caption{
    }
    \label{fig:glynn-best-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.35\textwidth}
  \centering
    \includegraphics[width=\linewidth]{glynn_is20016_2_edit.pdf}
    \caption{
    }
    \label{fig:glynn-best-ps}
  \end{subfigure}

  \caption[
    Best five time series in the \textit{zwf1$\Delta$} dataset and
    their periodograms,
    ranked by the quality of oscillation based on the maximum power in the periodogram \parencite{glynnDetectingPeriodicPatterns2006a}
  ]{
    \textbf{(\ref{fig:glynn-best-ts})}
    Best five time series in the \textit{zwf1$\Delta$} dataset and
    \textbf{(\ref{fig:glynn-best-ps})}
    their periodograms,
    ranked by the quality of oscillation based on the maximum power in the periodogram \parencite{glynnDetectingPeriodicPatterns2006a}.
  }
  \label{fig:glynn-best}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.65\textwidth}
  \centering
    \includegraphics[width=\linewidth]{glynn_is20016_3_edit.pdf}
    \caption{
    }
    \label{fig:glynn-worst-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.35\textwidth}
  \centering
    \includegraphics[width=\linewidth]{glynn_is20016_4_edit.pdf}
    \caption{
    }
    \label{fig:glynn-worst-ps}
  \end{subfigure}

  \caption[
    Worst five time series in the \textit{zwf1$\Delta$} dataset and
    their periodograms,
    ranked by the quality of oscillation based on the maximum power in the periodogram \parencite{glynnDetectingPeriodicPatterns2006a}
  ]{
    \textbf{(\ref{fig:glynn-worst-ts})}
    Worst five time series in the \textit{zwf1$\Delta$} dataset and
    \textbf{(\ref{fig:glynn-worst-ps})}
    their periodograms,
    ranked by the quality of oscillation based on the maximum power in the periodogram \parencite{glynnDetectingPeriodicPatterns2006a}.
  }
  \label{fig:glynn-worst}
\end{figure}

Figs.\ \ref{fig:glynn-best}--\ref{fig:glynn-worst} suggest that the best- and worst-ranked time series by the quality of their oscillatory signals conformed to subjective judgements of quality.
Lowest-ranked time series resembled pure noise and led to periodograms with power equally spread across all frequencies, thus bringing down the height of the highest peak.
Conversely, highest-ranked time series resembled sinusoids and therefore led to periodograms with a strong power corresponding to the frequency of the sinusoid that would model the time series.
However, the reliability of the ranking method is called into question by the high ranks (ranks 2, 3) of time series with irregular oscillations relative to others (Fig.\ \ref{fig:glynn-best}).

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{glynn_is20016_5_edit.pdf}
  \caption[
    ROC curve of classifier based on \textcite{glynnDetectingPeriodicPatterns2006a} as the false discovery rate was varied, trained on the \textit{zwf1$\Delta$} dataset.
  ]{
    ROC curve of classifier based on \textcite{glynnDetectingPeriodicPatterns2006a} as the false discovery rate was varied, trained on the \textit{zwf1$\Delta$} dataset.
  }
  \label{fig:glynn-roc}
\end{figure}

The method described by \textcite{glynnDetectingPeriodicPatterns2006a} describes a false discovery rate that is used to control the proportion of oscillations that are oscillatory.
In testing multiple hypotheses, the false discovery rate is defined as the proportion of cases in which the null hypothesis is true among all hypotheses in which the test is declared significant.
Increasing the false discovery rate thus increases the proportion of time series classed as oscillating.

To assess the performance of this method as a classifier for rhythmicity detection, Fig.\ \ref{fig:glynn-roc} shows the receiver operating characteristic (ROC) curve created as the false discovery rate was varied.
The area under the ROC curve ($A = 0.762$) suggests that the classifier only performed modestly well, especially for a large ($n=425$) dataset of time series with a large variety of quality of oscillations.


\subsection{Rhythmicity detection using model fitting}
\label{subsec:analysis-classification-ar}

To identify an alternative method to classify oscillatory and non-oscillatory time series, I assessed the performance of a time series classification method based on the autoregressive model.
%The autoregressive model has been used before to characterise biological time series \parencite{zielinskiStrengthsLimitationsPeriod2014}.
This type of model is based on the assumption that each data point in the time series can be expressed as a linear combination of $P$ data points that precede it, thus smoothing the time series.
The advantage of the autoregressive model is that it leads to an analytical solution for the periodogram, thus potentially resolving the problem of a low-resolution periodogram based on the Fourier spectrum.

In particular, \textcite{jiaFrequencyDomainAnalysis2021} describes a model of stochastic gene expression in a dividing cell, which predicts oscillatory gene expression.
Autoregressive models were fitted to simulated time series of oscillatory gene expression, and its parameters used to define a power spectrum for the time series (section~\ref{subsec:methods-computational-ar}).


\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{jiaFrequencyDomainAnalysis2020_2ab_adapted}
  \caption[
    Power spectra analytically derived from fitting an autoregressive model to time series can be divided into four types
  ]{
    Power spectra (a) analytically derived from fitting an autoregressive model to time series (b) can be divided into four types.
    Type I lacks a local maximum and is denoted as lacking oscillations.
    Figure adapted from \textcite{jiaFrequencyDomainAnalysis2020}.
  }
  \label{fig:analysis-ar-classification}
\end{figure}

The resulting power spectra fell into four categories: one of which corresponded to a lack of oscillations, characterised by an absence of a local maximum in the power spectrum (Fig.\ \ref{fig:analysis-ar-classification}).
This method thus allows computing the frequency of the oscillation from the location of the peak in the periodogram and quality of the oscillation from the height of the peak.

% Change colour scheme to make it a bit less purple and more blue-orange, consistent with chapt 4?
\begin{figure}
  \centering
  % Remove birth events?  They are not needed to understand this and may confuse people.
  \begin{subfigure}[htpb]{0.6\textwidth}
   \centering
   \includegraphics[width=\textwidth]{timeseries_example_for_ar}
   \caption{
   }
   \label{fig:analysis-ar-timeseries}
  \end{subfigure}%
  \begin{subfigure}[htpb]{0.4\textwidth}
   \centering
   \includegraphics[width=\textwidth]{ar}
   \caption{
   }
   \label{fig:analysis-ar-periodogram}
  \end{subfigure}

  \caption{
    \textbf{(\ref{fig:analysis-ar-timeseries})}
    Sample time series (dark solid line), with a fitted autoregressive model (light solid line) of order 4 computed according to \textcite{jiaFrequencyDomainAnalysis2020}.
    \textbf{(\ref{fig:analysis-ar-periodogram})}
    Periodogram defined based on parameters of the autoregressive model.
    % The presence of a peak of height greater than 1 indicates that the time series is oscillatory.
    % Furthermore, the locations of peaks estimate the period of oscillation in the original time series.
  }
  \label{fig:analysis-ar}
\end{figure}

Fig.\ \ref{fig:analysis-ar} shows that the autoregressive model was able to correctly identify a time series as oscillatory, as evidenced by a peak in the periodogram.% of height greater than 1.
In addition, the periodogram correctly identifies a period of \SI{75.2}{\minute}.

\begin{table}
  \centering
  % https://tex.stackexchange.com/a/20295
  \begin{tabular}{l|l|c|c|c}
    \multicolumn{2}{c}{}&\multicolumn{2}{c}{Human-defined labels}&\\
    \cline{3-4}
    \multicolumn{2}{c|}{}&Positive&Negative&\multicolumn{1}{c}{Total}\\
    \cline{2-4}
    \multirow{2}{*}{Predicted by AR model}& Positive & 141 & 83 & 224\\
    \cline{2-4}
    & Negative & 124 & 77 & 201\\
    \cline{2-4}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{Total} & \multicolumn{1}{c}{265} & \multicolumn{1}{c}{160} & \multicolumn{1}{c}{425}\\
  \end{tabular}
  \caption{
    Confusion matrix to evaluate the performance of using the autoregressive model \parencite{jiaFrequencyDomainAnalysis2020} to detect rhythmicity in the \textit{zwf1$\Delta$} dataset.
  }
  \label{tab:analysis-ar-confusion-matrix}
\end{table}

To assess the performance of the use of the autoregressive model for rhythmicity detection across a dataset, I extended this method across the \textit{zwf1$\Delta$} dataset, treating Type I power spectra (Fig.\ \ref{fig:analysis-ar-classification}) as non-oscillatory.
The confusion matrix (table~\ref{tab:analysis-ar-confusion-matrix}) suggests that the method leads to poor accuracy ($a = 0.513$).

% MOVE THIS TO DISCUSSION?
Furthermore, unlike the classifier based on \textcite{glynnDetectingPeriodicPatterns2006a}, the autoregressive model does not having a parameter that can be adjusted to change the expected proportion of a dataset to be classified as oscillatory --- in other words, the tolerance of detecting rhythmicity cannot be adjusted.
Alternatively, the order of the model can be treated as a parameter, and model selection can be performed.
%Nevertheless, among the time series that the algorithm classifies as oscillatory, the algorithm accurately estimates the frequency of oscillations.
%[Need: figures to summarise my investigation on population.  Probably need to re-do this on some datasets because it is poorly documented in my notes.]


\subsection{Rhythmicity detection using machine learning}
\label{subsec:analysis-classification-ml}
% - Write results from classifier project (SVM, RF, etc.)


% [TODO] Worth re-doing the whole experiment using better data, e.g. the data I'm actually using in the biological results chapter.

As an alternative to the mathematical methods previously discussed, I trained a machine learning model --- specifically, a support vector classifier --- to classify oscillatory and non-oscillatory time series and identify features in the time series that discriminate between the two classes (appendix \ref{append:analysis-ml}).
%
% DESCRIPTION OF DATA
Specifically, I trained a support vector classifier on flavin time series from BY4741 cells  under \SI{10}{\gram~\litre^{-1}} glucose.
The dataset had 294 time series of 118 time points each.

% DATA PROCESSING

% [TODO] Repeat experiment with Butterworth filter, rather than sliding window detrending
To pre-process the data, I normalised each time series $x_{i} = x_{i,1}, x_{i,2}, \ldots , x_{i,j}, \ldots x_{i,n}$ by normalising it to produce a processed time series $z_{i} = z_{i,1}, z_{i,2}, \ldots , z_{i,j}, \ldots z_{i,n}$ as follows:

\begin{equation}
  z_{i,j} = \frac{x_{i,j} - \mu_{i}}{\sigma_{i}}
  \label{eq:analysis-stdscore}
\end{equation}

where $\mu_{i}$ is the mean value of $x_{i}$, and $\sigma_{i}$ is the standard deviation of $x_{i}$.
As a result, each normalised time series $z_{i}$ has a mean of 0 and a standard deviation of 1.

% LABELLING
Based on the normalised time series, I labelled them as non-oscillatory ($n_{0}=211$) and oscillatory ($n_{1}=83$).
% SPLITTING
From this input data, 150 time series formed the training set.

% FEATURISATION
% MODELS
\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{svm_feat_compare_edit.pdf}
  \caption[
  ]{
    (Left) Precision and (right) recall from five-fold cross-validation of support vector classifiers trained using different featurisation methods:
    using the time points as features,
    using the power values in the Fourier spectrum as features,
    and using \emph{catch22} features.
    As a control, the oscillatory and non-oscillatory labels were randomly assigned to the time series and the time points were used as features.
  }
  \label{fig:analysis-precision-recall}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{svm_1_edit.pdf}
  \caption[
    Mean values of each \texttt{catch22} feature across each cell
  ]{
    Precision-recall curve of binary classifier, using \textit{catch22} for featurisation and with a support vector classifier architecture (radial bias kernel, $\gamma = 1/22$, $C = 1$).
  }
  \label{fig:analysis-svc-pr}
\end{figure}

To determine the most effective way to featurise the data, I computed the precision and recall of support vector classifiers trained on data featurised using different methods.
Precision and recall were chosen as evaluation metrics due to the class imbalance, and the support vector classifiers were trained using a radial bias kernel, a kernel coefficient $\gamma = 1/N$, where $N$ is the number of features, and a regularisation parameter $C = 1$.

Fig.\ \ref{fig:analysis-precision-recall} suggests that featurisation using \textit{catch22} gave the best performance.

% ADAPTATION OF SVM: PREDICT PROBABILITIES

\begin{figure}
  \centering
  \begin{subfigure}[htpb]{0.5\textwidth}
   \centering
   \includegraphics[width=\textwidth]{svm_2_edit.pdf}
   \caption{
   }
   \label{fig:analysis-svc-proba-historgram-model}
  \end{subfigure}%
  \begin{subfigure}[htpb]{0.5\textwidth}
   \centering
   \includegraphics[width=\textwidth]{svm_scramble_2_edit.pdf}
   \caption{
   }
   \label{fig:analysis-svc-proba-histogram-scramble}
  \end{subfigure}

  \caption{
    \textbf{(\ref{fig:analysis-svc-proba-historgram-model})}
    Histogram of probabilities of whether a time series in the test data set is classified as oscillatory by the SVC, and as a control,
    \textbf{(\ref{fig:analysis-svc-proba-histogram-scramble})}
    with labels randomly re-assorted to time series.
  }
  \label{fig:analysis-svc-proba-histogram}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{svm_3_edit.pdf}

  \caption{
    Sample test set time series arranged by probability that each is oscillatory, as predicted by the support vector classifier.
  }
  \label{fig:analysis-svc-proba-gallery}
\end{figure}

To predict the probability that each time series is oscillatory to account for uncertainties, in contrast to assigning each test time series to one of two categories, I extended the support vector classifier to predict probabilities.
This was performed using Platt scaling \parencite{plattProbabilisticOutputsSupport1999}, as implemented by \texttt{scikit-learn}.
Fig.\ \ref{fig:analysis-svc-proba-histogram} shows that the distribution of probabilities falls in a U-shape.
This distribution is desirable because it indicates that the classifier is for the most part `certain' whether a time series is oscillatory.
In addition, these probabilities can also serve as a score for the quality of oscillations, as illustrated by Fig.\ \ref{fig:analysis-svc-proba-gallery}.


In sum,
a simple machine learning architecture is sufficient in classifying noisy biological time series into oscillatory and non-oscillatory categories, complete with probabilities.
This method was also able to identify which time series feature was the most important in distinguishing between oscillatory and non-oscillatory time series.


%\section[Characterisation]{Characterisation: I have one time series --- what properties does it have?}
\section{Period estimation using the autocorrelation function}
\label{sec:analysis-characterisation}

Characterising features of oscillatory time series is important as it provides a quantitative measure of how yeast metabolic cycles respond to genetic nutrient perturbations.
%The characteristics that are important to my investigation of the yeast metabolic cycle include the period and the amplitude.
The period of an oscillatory time series is the easiest characteristic to compute a fixed number for because it is well-defined.
%Similarly, the amplitude is easy to estimate, based on fitting a cosine to the signal, but are inadequate if oscillations are asymmetrical.
%However, due to the detrending methods I used, information about the amplitude is lost, and therefore it must be indirectly inferred from the magnitude of noise, assuming that the sources of noise are constant across time series.

% TODO: Edit this paragraph.  It's a Frankenstein's creation of several others that were deleted.
In this section, I show that the autocorrelation function can be adapted to characterise the period and noise properties of populations of both sinusoid time series and time series and asymmetrical oscillations.
The cross-correlation function used in this chapter is adapted from \textcite{pietschDeterminingGrowthRates2023} (section~\ref{subsec:methods-computational-xcf}).
I first do so with synthetic time series with known properties and adapt the methods to my data that exhibit similar oscillations as the synthetic time series.
To understand the effect of the autocorrelation function on biological time series, I modelled time series flavin fluorescence oscillations using the harmonic oscillator and modelled time series of histone 2B abundance patterns using the FitzHugh-Nagumo oscillator \parencite{fitzhughImpulsesPhysiologicalStates1961} (section~\ref{subsec:methods-computational-synthetic}).

\subsection{Synthetic data}
\label{subsubsec:analysis-characterisation-synthetic}

\subsubsection{Harmonic oscillator: effect of noise parameters}
\label{subsubsec:analysis-characterisation-acf-sinusoid}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{sinusoids_outofphase}
    \caption{
    }
    \label{fig:acf-sinusoids-nonoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{sinusoids_outofphase_acf_corrected}
    \caption{
    }
    \label{fig:acf-sinusoids-nonoise-acf}
  \end{subfigure}

  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{verynoisysinusoids_outofphase}
    \caption{
    }
    \label{fig:acf-sinusoids-gausnoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{verynoisysinusoids_outofphase_acf}
    \caption{
    }
    \label{fig:acf-sinusoids-gausnoise-acf}
  \end{subfigure}

  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p05_mean}
    \caption{
    }
    \label{fig:acf-sinusoids-gillnoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p05_acf}
    \caption{
    }
    \label{fig:acf-sinusoids-gillnoise-acf}
  \end{subfigure}

  \caption{
    %Effect of type of noise on the autocorrelation function.
    \textbf{(\ref{fig:acf-sinusoids-nonoise-ts})} Sample sinusoids without noise, and 
    \textbf{(\ref{fig:acf-sinusoids-nonoise-acf})} its autocorrelation function.
    %
    \textbf{(\ref{fig:acf-sinusoids-gausnoise-ts})} Sample sinusoids with Gaussian noise defined by drawing samples from $\mathcal{N}(0,\sigma^{2}=3)$, and 
    \textbf{(\ref{fig:acf-sinusoids-gausnoise-acf})} its autocorrelation function.
    %
    \textbf{(\ref{fig:acf-sinusoids-gillnoise-ts})} Sample sinusoids of with Gillespie noise ($k_{0} = 5$ and $d_{0} = 0.05$), and 
    \textbf{(\ref{fig:acf-sinusoids-gillnoise-acf})} its autocorrelation function.
    Red line is defined by $y = \me^{-2d_{0}T}$, where $T$ represents the lag in units of period of the sinusoids.
    %
    For each case, the frequency of the sinusoids was 0.03, and there were 100 repeats, randomly out-of-phase.
  }
  \label{fig:acf-sinusoids}
\end{figure}

To compare the effect of Gaussian noise and Gillespie noise on the autocorrelation function, I computed the population autocorrelation from a population of sinusoids augmented with the two types of noise.

% [Show working -- i.e. the mathematical derivations that make me expect results?]
% [Equations -- clear lines before/after, rather than having them in-line?]

Fig.\ \ref{fig:acf-sinusoids-nonoise-acf} shows that the autocorrelation function computed from a population of out-of-phase sinusoids can be modelled by the function $y(T) = cos(2 \pi T)$, where $f$ is the frequency of the sinusoids and $T$ is the lag in units of the periods of the sinusoid.
This relationship can be confirmed by deriving from the definition of the autocorrelation function used in this chapter (mathematical derivation in appendix ...).

Following this, Fig.\ \ref{fig:acf-sinusoids-gausnoise-acf} shows that the addition of Gaussian noise preserved the point $y(0) = 1$, but the amplitude of the cosine that models the autocorrelation function is reduced.
Furthermore, the variation of the autocorrelation function among time series at long lags is increased, evidenced by the interquartile range.
This variation is because less data was used to compute the autocorrelation function at long lags (mathematical derivation in appendix ...).

Gillespie noise is based on the birth-death process, and its two parameters control noise parameters (section~\ref{subsec:methods-computational-synthetic}).
Specifically, given a birth rate $k_{0}$ and a death rate $d_{0}$, the noise has a standard deviation of noise amplitude $A = \sqrt{k_{0}/d_{0}}$ and noise timescale $\tau = 1/d_{0}$.
% [Show working -- i.e. the mathematical derivations that make me expect this exponential fit?]
% Though Peter said this `is not trivial'.
Fig.\ \ref{fig:acf-sinusoids-gillnoise-acf} shows that when Gillespie noise was added to the sinusoids, the medium autocorrelation followed the exponential decay function $y = \me^{-2d_{0}T}$, where $T$ represents lag.
In addition, the location of the peaks --- corresponding to the period of the sinusoid --- were preserved.
This figure thus suggests that the death rate $d_{0}$ parameter of Gillespie noise controlled the shape of the autocorrelation function.


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p5_mean.png}
    \caption{
    }
    \label{fig:acf-noisetimescale-highd0-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p5_acf.png}
    \caption{
    }
    \label{fig:acf-noisetimescale-highd0-acf}
  \end{subfigure}

  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p005_mean.png}
    \caption{
    }
    \label{fig:acf-noisetimescale-lowd0-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k5_d0p005_acf.png}
    \caption{
    }
    \label{fig:acf-noisetimescale-lowd0-acf}
  \end{subfigure}

  \caption[
    Effect of death rate of Gillespie noise on the autocorrelation function.
  ]{
    %Effect of death rate ($d_{0}$) of Gillespie noise on the autocorrelation function.
    \textbf{(\ref{fig:acf-noisetimescale-highd0-ts})} Sample sinusoids with Gillespie noise ($k_{0} = 5$ and $d_{0} = 0.5$), and 
    \textbf{(\ref{fig:acf-noisetimescale-highd0-acf})} its autocorrelation function.
    %
    \textbf{(\ref{fig:acf-noisetimescale-lowd0-ts})} Sample sinusoids with Gillespie noise ($k_{0} = 5$ and $d_{0} = 0.005$), and 
    \textbf{(\ref{fig:acf-noisetimescale-lowd0-acf})} its autocorrelation function.
    %
    Red lines are defined by $y = \me^{-2d_{0}T}$, where $T$ represents the lag in units of period of the sinusoids.
    %
    For each case, the frequency of the sinusoids was 0.03, and there were 100 repeats, randomly out-of-phase.
  }
  \label{fig:acf-noisetimescale}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
  \centering
    \includegraphics[width=\linewidth]{acf_fit_example.png}
    \caption{
    }
    \label{fig:acf-noisetimescale-effect-fit}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
  \centering
    \includegraphics[width=\linewidth]{deathrate_vs_decay.png}
    \caption{
    }
    \label{fig:acf-noisetimescale-effect-relationship}
  \end{subfigure}

  \caption{
    %Characterising the autocorrelation function to quantify the effect of Gillespie noise timescale.
    \textbf{(\ref{fig:acf-noisetimescale-effect-fit})} Fitting exponential decay functions to estimate $d_{0}$ from the autocorrelation function.
    \textbf{(\ref{fig:acf-noisetimescale-effect-relationship})} The relationship between $d_{0}$ and the decay rate $D$ found from fitting exponential decay functions to the mean autocorrelation function, the peaks, and the troughs of this mean function.
    Here, $k_{0}$ was held constant at 5.
  }
  \label{fig:acf-noisetimescale-effect}
\end{figure}

To quantify the effect of the timescale of noise, defined by $\tau = 1/d_{0}$, I varied the death rate parameter $d_{0}$ when generating Gillespie noise that was to be added to the sinusoids.

Fig.\ \ref{fig:acf-noisetimescale} shows that a higher death rate decreased the decay timescale of the autocorrelation function (Fig.\ \ref{fig:acf-noisetimescale-highd0-ts}), while a lower death rate introduced long-term trends in the simulated signals (Fig.\ \ref{fig:acf-noisetimescale-highd0-ts}).
A lower death rate also increased the decay timescale for the autocorrelation function and increased the variation between autocorrelation functions between replicates (Fig.\ \ref{fig:acf-noisetimescale-lowd0-acf}).

To show how $d_{0}$ can be estimated from the autocorrelation function, I fit exponential decay functions of the form $y = (1-C)\me^{-DT}+C$ to the mean autocorrelation, the peaks of the mean function, and the troughs of the mean functions using non-linear least squares fitting, defining $C$ and $D$ as variable parameters (Fig.\ \ref{fig:acf-noisetimescale-effect-fit}).
Subsequently, Fig.\ \ref{fig:acf-noisetimescale-effect-relationship} suggests that the decay rates $D$ of the autocorrelation function indeed increases linearly with $d_{0}$
In other words, the death rate $d_{0}$ controls the decay rate $D$ of the autocorrelation function.
Conversely, if $D$ can be estimated from the autocorrelation function, then the noise timescale $\tau$ of the time series can be estimated.


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k25_d0p05_mean.png}
    \caption{
    }
    \label{fig:acf-noiseamplitude-highk0-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k25_d0p05_acf.png}
    \caption{
    }
    \label{fig:acf-noiseamplitude-highk0-acf}
  \end{subfigure}

  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k1_d0p05_mean.png}
    \caption{
    }
    \label{fig:acf-noiseamplitude-lowk0-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{gillespie_k1_d0p05_acf.png}
    \caption{
    }
    \label{fig:acf-noiseamplitude-lowk0-acf}
  \end{subfigure}

  \caption[
    Effect of birth rate of Gillespie noise on the autocorrelation function.
  ]{
    %Effect of birth rate ($k_{0}$) of Gillespie noise on the autocorrelation function.
    \textbf{(\ref{fig:acf-noiseamplitude-highk0-ts})} Sample sinusoids with Gillespie noise ($k_{0} = 25$ and $d_{0} = 0.05$), and 
    \textbf{(\ref{fig:acf-noiseamplitude-highk0-acf})} its autocorrelation function.
    %
    \textbf{(\ref{fig:acf-noiseamplitude-lowk0-ts})} Sample sinusoids with Gillespie noise ($k_{0} = 1$ and $d_{0} = 0.05$), and 
    \textbf{(\ref{fig:acf-noiseamplitude-lowk0-acf})} its autocorrelation function.
    %
    Red lines are defined by $y = \me^{-2d_{0}T}$, where $T$ represents the lag in units of period of the sinusoids.
    %
    For each case, the frequency of the sinusoids was 0.03, and there were 100 repeats, randomly out-of-phase.
  }
  \label{fig:acf-noiseamplitude}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{birthrate_vs_ydispl.png}
  \caption[
    Quantifying the effect of Gillespie noise amplitude on the autocorrelation function
  ]{
    The relationship between the noise amplitude and the $y$-displacement $C$ found from fitting exponential decay functions to the mean autocorrelation function, the peaks, and the troughs of this mean function.
    Here, $d_{0}$ was held constant at 0.05.
  }
  \label{fig:acf-noiseamplitude-effect}
\end{figure}


To quantify the effect of the noise amplitude, defined by $A = \sqrt{k_{0}/d_{0}}$, I varied the birth rate parameter $k_{0}$ when generating Gillespie noise that was to be added to the sinusoids (Fig.\ \ref{fig:acf-noiseamplitude}).

Fig.\ \ref{fig:acf-noiseamplitude} shows that a lower birth rate decreased the amplitude of noise (Fig.\ \ref{fig:acf-noiseamplitude-lowk0-ts}) and decreased the variation between replicate autocorrelation functions (Fig.\ \ref{fig:acf-noiseamplitude-lowk0-acf}).
Conversely, a higher birth rate increased the amplitude of noise (Fig.\ \ref{fig:acf-noiseamplitude-highk0-ts}) and increased the variation between replicate autocorrelation functions (Fig.\ \ref{fig:acf-noiseamplitude-highk0-acf}).

To show how the noise amplitude can be estimated from the autocorrelation function, I fit exponential decay functions as in Fig.\ \ref{fig:acf-noisetimescale-effect-fit}.
Fig.\ \ref{fig:acf-noiseamplitude-effect} suggests that the $y$-displacements $C$ of the exponential fits to peaks and troughs converged to 0 as $k_{0}/d_{0}$ increased, showing that the amplitude of the oscillations in the autocorrelation function decreases as the noise amplitude increases.
In other words, the birth rate $d_{0}$ controls the $y$-displacement parameter $C$ of the autocorrelation function, which is a proxy for the function's amplitude.
Conversely, if $C$ can be estimated from the autocorrelation function, then the noise amplitude $A$ of the time series can be estimated.

% [Move to conclusion?]
% To conclude, if a population of replicate oscillatory time series is modelled with the sum of sinusoids and Gillespie noise, then the birth rate and death rate can control the shape of the autocorrelation function.
% The death rate controls the timescale of noise and thus how fast the autocorrelation decays as lag increases.
% The birth rate controls the amplitude of noise and thus controls how robust the autocorrelation function is.


\subsubsection{FitzHugh-Nagumo oscillator: effect of oscillator shape}
\label{subsubsec:analysis-characterisation-acf-fhn}


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_meanplot}
    \caption{
    }
    \label{fig:acf-fhn-gillnoise-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_acf}
    \caption{
    }
    \label{fig:acf-fhn-gillnoise-acf}
  \end{subfigure}

  \caption{
    %The autocorrelation function of FitzHugh-Nagumo oscillators with Gillespie noise.
    \textbf{(\ref{fig:acf-fhn-gillnoise-ts})} Sample FitzHugh-Nagumo oscillators ($RI_{\mathrm{ext}}$ = 0.4, $\tau$ = 12.5, $a$ = 0.7, $b$ = 0.82) with Gillespie noise ($k_{0} = 5$ and $d_{0} = 0.05$), and 
    \textbf{(\ref{fig:acf-fhn-gillnoise-acf})} its autocorrelation function.
    Red line is defined by $y = \me^{-2d_{0}T}$, where $T$ represents the lag in units of period of the sinusoids.
    %
    There were 100 repeats, randomly out-of-phase.
  }
  \label{fig:acf-fhn}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_expofit}
    \caption{
    }
    \label{fig:acf-fhn-noiseparams-fit}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_highnts_expofit}
    \caption{
    }
    \label{fig:acf-fhn-noiseparams-fit-highnts}
  \end{subfigure}

  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_deathrate_vs_decay.png}
    \caption{
    }
    \label{fig:acf-fhn-noiseparams-noisetimescale}
  \end{subfigure}%
  \begin{subfigure}[t]{0.45\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fhn_birthrate_vs_ydispl.png}
    \caption{
    }
    \label{fig:acf-fhn-noiseparams-noiseamplitude}
  \end{subfigure}

  \caption{
    %Quantifying the effect of Gillespie noise parameters on the autocorrelation function of FitzHugh-Nagumo oscillators.
    \textbf{(\ref{fig:acf-fhn-noiseparams-fit})}
    Fitting exponential decay functions of the form $y = (1-C)\me^{-DT}+C$, with $C$ and $D$ as variable parameters, to the autocorrelation function, generated with $k_{0}=5$ and $d_{0}=0.05$, and
    \textbf{(\ref{fig:acf-fhn-noiseparams-fit-highnts})}
    with $k_{0}=5$ and $d_{0}=0.005$.
    \textbf{(\ref{fig:acf-fhn-noiseparams-noisetimescale})}
    The relationship between $d_{0}$ and the decay rate $D$ ($k_{0}=5$).
    \textbf{(\ref{fig:acf-fhn-noiseparams-noiseamplitude})}
    The relationship between $k_{0}/d_{0}$ and $y$-displacement $C$ of the exponential fit ($d_{0}=0.05$)
  }
  \label{fig:acf-fhn-noiseparams}
\end{figure}

To test whether Gillespie noise parameters can be estimated from the autocorrelation function computed from oscillations of a different shape, I repeated the exponential fitting in section~\ref{subsubsec:analysis-characterisation-acf-sinusoid} on the FitzHugh-Nagumo oscillator.

Fig.\ \ref{fig:acf-fhn-gillnoise-acf} shows that when the oscillator had a different shape, the shape of the autocorrelation function changed: i.e.\ the waves in the autocorrelation function were more pointed.
In addition, fitting an exponential decay function to the autocorrelation function to estimate noise parameters was less reliable, particularly with high noise timescales (Fig.\ \ref{fig:acf-fhn-noiseparams-fit-highnts}).
As a consequence, estimating the noise timescale $\tau$ from the decay rate $D$ of the exponential decay function produced a greater range of uncertainty (Fig.\ \ref{fig:acf-fhn-noiseparams-noisetimescale}).
However, estimating the noise amplitude $A$ based on the y-displacement $C$ of the exponential decay function remained as reliable as the sinusoid oscillator case (Fig.\ \ref{fig:acf-fhn-noiseparams-noiseamplitude}).


\subsection{Real data}
\label{subsubsec:analysis-characterisation-real}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{26643_ts.png}
    \caption{
    }
    \label{fig:acf-sinusoid-biol-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{fy4_26643_plots_06.png}
    \caption{
    }
    \label{fig:acf-sinusoid-biol-acf}
  \end{subfigure}

  \caption{
    \textbf{(\ref{fig:acf-sinusoid-biol-ts})}
    Sample time series of flavin autofluorescence.
    \textbf{(\ref{fig:acf-sinusoid-biol-acf})}
    Autocorrelation function across a population of time series of flavin autofluorescence.
  }
  \label{fig:acf-sinusoid-biol}
\end{figure}

From the relationships shown in Figs.\ \ref{fig:acf-noiseamplitude-effect} and \ref{fig:acf-noisetimescale-effect}, noise parameters from the autocorrelation functions of real signals may be deduced.
Fig.\ \ref{fig:acf-sinusoid-biol} shows ...(INSERT RESULT OF NEW INVESTIGATION HERE).


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
  \centering
  % TODO: Edit out the flavin bits
    \includegraphics[width=\linewidth]{htb2mCherry_26643_plots_purple_01.pdf}
    \caption{
    }
    \label{fig:acf-fhn-biol-ts}
  \end{subfigure}%
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{htb2mCherry_26643_plots_mCh_06.png}
    \caption{
    }
    \label{fig:acf-fhn-biol-acf}
  \end{subfigure}

  \caption{
    \textbf{(\ref{fig:acf-fhn-biol-ts})}
    Sample time series of histone 2B abundance (pink).
    \textbf{(\ref{fig:acf-fhn-biol-acf})}
    Autocorrelation function across a population of time series of histone 2B abundance.
  }
  \label{fig:acf-fhn-biol}
\end{figure}

From the relationships shown in Figs.\ \ref{fig:acf-fhn-noiseparams-noisetimescale} and \ref{fig:acf-fhn-noiseparams-noiseamplitude}, noise parameters from the autocorrelation functions of real signals may be deduced.
Fig.\ \ref{fig:acf-fhn-biol} shows ...(INSERT RESULT OF NEW INVESTIGATION HERE).


%\section[Correlation]{Correlation: I have two signals from the same cell --- what are their relationships to each other?}
\section{Detection of synchrony}
\label{sec:analysis-correlation}

To test a method to quantify the temporal lag between flavin autofluorescence oscillations and HTB2::mCherry levels in a population of cells, I computed the cross-correlation functions of a population of sinusoid and FitzHugh-Nagumo oscillators.
Cross-correlation has been used to investigate the relationship between the expression levels of two genes in a model feed-forward loop \parencite{dunlopRegulatoryActivityRevealed2008},
and to investigate the relationship between instantaneous growth rate and the expression of \textit{lac} genes OR of enzymes in central metabolism across a population of \textit{E. coli} cells \parencite{kivietStochasticityMetabolismGrowth2014}.
Importantly, \textcite{kivietStochasticityMetabolismGrowth2014} employed a single-cell microfluidics experiment with time-lapse microscopy, and therefore their analysis of a population of time series is applicable in this chapter.

\subsection{Synthetic data}
\label{subsubsec:analysis-correlation-synthetic}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{sinusoid_and_fitzhughnagumo_nonoise.png}
    \caption{
    }
    \label{fig:xcf-nonoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{randomshift_sinusoid_fitzhughnagumo_xcf.png}
    \caption{
    }
    \label{fig:xcf-nonoise-xcf}
  \end{subfigure}

  \begin{subfigure}[t]{0.6\textwidth}
  \centering
    \includegraphics[width=\linewidth]{sinusoid_and_fitzhughnagumo_gillnoise.png}
    \caption{
    }
    \label{fig:xcf-gillnoise-ts}
  \end{subfigure}%
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{randomshift_sinusoid_fitzhughnagumo_gillnoise_xcf.png}
    \caption{
    }
    \label{fig:xcf-gillnoise-xcf}
  \end{subfigure}

  \caption{
    %Using the cross-correlation function to evaluate the shift of one synthetic time series relative to another with a different shape.
    \textbf{(\ref{fig:xcf-nonoise-ts})}
    (Blue) Sample sinusoid ($f = 0.026$) and (orange) FitzHugh-Nagumo oscillator ($RI_{\mathrm{ext}}$ = 0.4, $\tau$ = 12.5, $a$ = 0.7, $b$ = 0.82) of the same frequency and without noise, and
    \textbf{(\ref{fig:xcf-nonoise-xcf})}
    the cross-correlation function of the FitzHugh-Nagumo oscillators with respect to the sinusoids.
    %
    \textbf{(\ref{fig:xcf-gillnoise-ts})}
    (Blue) Sample sinusoid and (orange) FitzHugh-Nagumo oscillator with same parameters as \ref{fig:xcf-nonoise-ts}, but with Gillespie noise ($d_{0} = 0.05$, $k_{0} = 5$), and
    \textbf{(\ref{fig:xcf-gillnoise-xcf})}
    the cross-correlation function of the FitzHugh-Nagumo oscillators with respect to the sinusoids.
    %
    For each case, there were 400 repeats, randomly out-of-phase.
  }
  \label{fig:xcf}
\end{figure}

Fig.\ \ref{fig:xcf-nonoise-xcf} shows that the cross-correlation function correctly indicated the degree to which the FitzHugh-Nagumo oscillators were shifted forward with respect to the sinusoids.
This shift was evidenced by the degree to which the central peak of the cross-correlation function was shifted to the right, in contrast to the situation in which the cross-correlation at lag 0 is 1 if the peaks of the two oscillators coincide.
Furthermore, Fig.\ \ref{fig:xcf-gillnoise-xcf} suggests that even with strong Gillespie noise, the lag between the two oscillators could still be deduced from the cross-correlation function.


\subsection{Real data}
\label{subsubsec:analysis-correlation-real}

% TODO: Replace with pyruvate?  More dramatic.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.9\textwidth}
  \centering
    \includegraphics[width=\linewidth]{htb2mCherry_26643_plots_purple_01.pdf}
    \caption{
      Sample time series of flavin autofluorescence (purple) and histone 2B abundance (pink).
    }
    \label{fig:xcf-biol-ts}
  \end{subfigure}

  \begin{subfigure}[t]{0.7\textwidth}
  \centering
    \includegraphics[width=\linewidth]{xcf_edit.pdf}
    \caption{
      Cross-correlation function of the histone 2B abundance time series with respect to the flavin autofluorescence time series.
    }
    \label{fig:xcf-biol-xcf}
  \end{subfigure}

  \caption{
    Using the cross-correlation function to evaluate the shift of histone 2B abundance traces with respect to flavin autofluorescence traces.
  }
  \label{fig:xcf-biol}
\end{figure}

I can thus use this method to quantify the shift of the cell division cycle relative to the yeast metabolic cycle, therefore assessing the relationship between the two oscillators.
Figure~\ref{fig:xcf-biol} shows such as example, which demonstrates that the histone 2B oscillations succeed the flavin autofluorescence oscillations by an average of \SI{5}{\minute}.


\section{Discussion}
\label{sec:analysis-discussion}

In this chapter, I describe the process of analysing populations of single-cell data of flavin autofluorescence oscillation and of histone 2B localisation time series, broken down into five main steps.
These steps consist of: cleaning data, classification of oscillatory time series, characterisation of features of oscillatory time series, correlation of two related time series, and clustering of similar time series in a dataset.

In each step, there are several methods to choose from, each with its own merits and caveats, and judgement calls must be made.
Additionally, combining different methods to answer the same time series question is often needed to provide a good picture of the quantity being measured.
In cleaning data, the analyst must decide on a threshold to filter out useless data from the useful ones, but without compromising on the size of the resulting dataset so the remaining dataset is still useful.
In classification of oscillatory time series, rhythmicity detection depends upon defining a threshold that separates `oscillatory' from `non-oscillatory' time series.
This is true whether or not the rhythmicity detection method relies on machine learning.
Furthermore, it is difficult to do this with noisy and relatively short (on the order of 10 oscillations) time series.
It is easier to numerically characterise some properties of oscillatory time series than others, but all methods give error intervals.
And finally, clustering also depends on defining parameters that control the number of groups found in a dataset.

Solving mathematical and computational problems for each task is an intellectual investigation in its own right, and opens more questions than answers.
%And so future directions include things like using Bayesian method to classify oscillations, or whether better data or better featurisation helps.
Nevertheless, combining all five main steps can form a powerful analysis workflow that can be adapted to the analysis of other large datasets of time series, biological or not.
I illustrate the use of such an example of an analysis workflow in chapter~\ref{ch:biology}, when I apply it to analyse biological results.
Constructing an analysis workflow can be an interesting and challenging software engineering problem in its own right.

% Not sure how useful this subsection is...
% And it's also a bit dangling
\subsection{Combining methods to get a picture of periodicity}
\label{subsec:analysis-characterisation-combined}

Each periodicity-estimation method has a limited ability to estimate the period of short, noisy biological time series; therefore, it is often useful to combine several such methods to produce a picture of the periodicity of oscillatory time series.
For example, \textcite{potvin-trottierSynchronousLongtermOscillations2016} combines the autocorrelation function and the Fourier transform to study the changes in the periodicity of a modified model of the repressilator.
%Thus, in my subsequent analysis of biological data, I combine the autocorrelation function and the Fourier spectrum.

% [FIGURE: SHOWS A GOOD TIME SERIES, FFT, ACF, AR, AND THE MOST PROBABLY OSCILLATION FREQUENCY FROM EACH]

% [FIGURE: THE SAME AS ABOVE BUT LOOKING AT A POPULATION OF TIME SERIES.  THESE TRAJECTORIES SHOULD HAVE BEEN COMPUTED INDIVIDUALLY FOR EACH TIME SERIES.  THE CAPTION SHOULD EXPLAIN THE MEAN/MEDIAN AND ERROR RANGES.]
