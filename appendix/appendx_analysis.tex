\chapter{Time series analysis}
\label{append:analysis}

\section{catch22 features}
\label{append:analysis-catch22}

Table~\ref{tab:catch22} lists \emph{catch22} features.

\begin{table}[htbp]
  \small
  \centering
  \begin{tabularx}{\linewidth}{bbS}
    \toprule
    Feature name & Description \\
    \midrule
    \texttt{DN\_\-HistogramMode\_\-5} & Mode of z-scored distribution (5-bin histogram) \\
    \texttt{DN\_\-HistogramMode\_\-10} & Mode of z-scored distribution (10-bin histogram) \\
    \texttt{SB\_\-BinaryStats\_\-mean\_\-longstretch1} & Longest period of consecutive values above the mean  \\
    \texttt{DN\_\-OutlierInclude\_\-p\_\-001\_\-mdrmd} & Time intervals between successive extreme events above the mean \\
    \texttt{DN\_\-OutlierInclude\_\-n\_\-001\_\-mdrmd} & Time intervals between successive extreme events below the mean \\
    \texttt{first\_\-1e\_\-ac} & First 1/e crossing of autocorrelation function \\
    \texttt{firstMin\_\-acf} & First minimum of autocorrelation function \\
    \texttt{SP\_\-Summaries\_\-welch\_\-rect\_\-area\_\-5\_\-1} & Total power in lowest fifth of frequencies in the Fourier power spectrum \\
    \texttt{SP\_\-Summaries\_\-welch\_\-rect\_\-centroid} & Centroid of the Fourier power spectrum \\
    \texttt{FC\_\-LocalSimple\_\-mean3\_\-stderr} & Mean error from a rolling 3-sample mean forecasting \\
    \texttt{CO\_\-trev\_\-1\_\-num} & Time-reversibility statistic, $\langle(x_{t+1} - x_t)^3\rangle_t$ \\
    \texttt{CO\_\-HistogramAMI\_\-even\_\-2\_\-5} & Automutual information, $m = 2, \tau = 5$ \\
    \texttt{IN\_\-AutoMutualInfoStats\_\-40\_\-gaussian\_\-fmmi} & First minimum of the automutual information function \\
    \texttt{MD\_\-hrv\_\-classic\_\-pnn40} & Proportion of successive differences exceeding $0.04\sigma$ \\
    \texttt{SB\_\-BinaryStats\_\-diff\_\-longstretch0} & Longest period of successive incremental decreases \\
    \texttt{SB\_\-MotifThree\_\-quantile\_\-hh} & Shannon entropy of two successive letters in equiprobable 3-letter symbolization \\
    \texttt{FC\_\-LocalSimple\_\-mean1\_\-tauresrat} & Change in correlation length after iterative differencing \\
    \texttt{CO\_\-Embed2\_\-Dist\_\-tau\_\-d\_\-expfit\_\-meandiff} & Exponential fit to successive distances in 2-d embedding space \\
    \texttt{SC\_\-FluctAnal\_\-2\_\-dfa\_\-50\_\-1\_\-2\_\-logi\_\-prop\_\-r1} & Proportion of slower timescale fluctuations that scale with DFA (50\% sampling) \\
    \texttt{SC\_\-FluctAnal\_\-2\_\-rsrangefit\_\-50\_\-1\_\-logi\_\-prop\_\-r1} & Proportion of slower timescale fluctuations that scale with linearly rescaled range fits \\
    \texttt{SB\_\-TransitionMatrix\_\-3ac\_\-sumdiagcov} & Trace of covariance of transition matrix between symbols in 3-letter alphabet \\
    \texttt{PD\_\-PeriodicityWang\_\-th0\_\-01} & Periodicity measure of \textcite{wangStructureBasedStatisticalFeatures2007}   \\
    \bottomrule \\
  \end{tabularx}
  \caption[
    \textit{catch22} features
  ]{
    \textit{catch22} features, adapted from \textcite{lubbaCatch22CAnonicalTimeseries2019}.
  }
  \label{tab:catch22}
\end{table}

\section{UMAP hyperparameters}
\label{append:analysis-umap}

UMAP \parencite{mcinnesUMAPUniformManifold2020} is a dimension reduction method which aims to find a global manifold structure of the input observations and computes a low-dimension embedding that preserves the structure of the manifold.

UMAP has several hyperparameters, of which four have major effects on the embedding:
\begin{itemize}
  \item The \emph{number of neighbours} ($n$) to consider when approximating the local metric controls how the method balances local and global structure in the data.
        With low values of this parameter, the algorithm concentrates on very local structure, potentially to the detriment of the big picture.
        As the value increases, the algorithm `glues' more nodes together to form clusters.
  \item The \emph{largest embedding dimension} ($d$) controls the number of dimensions the data is reduced to.
        In other words, it controls whether the resulting map is one-dimensional, two-dimensional, three-dimensional, or of higher dimensions.
  \item The \emph{minimal distance} ($\mathrm{min\_dist}$) controls the desired separation between close points in the embedding space.
        Specifically, this parameter controls how tightly the algorithm is allowed to pack points together.
        With low values, the visualisation forms `clumps'.
  \item The previous hyperparameters are numerical, but the \emph{metric} hyperparameter instead specifies the distance metric that is used to compute distances in the ambient space of the input data.
        For example, this metric can be the Euclidean distance, the cosine distance, or other metrics used to compute the distances between two vectors of numerical data.
\end{itemize}

% MOVE TO DISCUSSION?
% However, UMAP has several caveats.
% It lacks strong interpretability, namely: dimensions do not mean anything, unlike in principal component analysis (PCA).
% Additionally, it assumes that the data has a manifold structure.
% So it tends to find manifold structure within the noise of a dataset, like how humans find constellations among stars.
% If more data are sampled, then the amount of structure from noise will tend to decrease.
% It assumes that local distance is more important than long-range distances, like t-SNE.
% And finally, approximations were made to improve computational efficiency.


\section{Classification pipeline}
\label{append:analysis-ml}

% OVERVIEW OF MACHINE LEARNING
% (needed to make the rest of the section make sense)
In machine learning, classification is defined as the process of identifying a category that a piece of input data belongs to.
In this section, the classification task is identifying whether a time series (input data) is oscillatory (belongs to one category of two) or non-oscillatory (belongs to the other category of two).

% Is this necessary?  It seems too basic.  Dump into appendix?
A typical classification pipeline can be described by the following steps:
\begin{enumerate}
  \item \emph{Pre-processing of data:} Input data is cleaned or normalised.
        For example, to classify oscillatory time series, the input time series may be normalised to give similar dynamic ranges.
  \item \emph{Labelling:} Each piece of input data has a label assigned to it to denote which category it belongs to.
        For example, to classify oscillatory time series, a human can subjective assign the label `0' for non-oscillatory time series and `1' for oscillatory time series, for a total of two categories.
  \item \emph{Featurisation:} Input data converted to feature vectors in the process of featurisation.
        This process uses domain knowledge related to the type or origin of the data to define characteristics of the data that may be useful for classification.
  \item \emph{Train-test split:} The input data set is then randomly divided into a training data set and a test data set.
  \item \emph{Training of model:} The machine learning model is then fit on the (featurised) training data set and its labels to fit parameters in the model.
  \item \emph{Evaluation of model on test dataset:}
        The model, trained on the training dataset, is used to predict the labels of data in the (featurised) test data set.
        The performance of the model is then evaluated on the test data set.
        This evaluation is based on computing quantities that express how well the model assigns labels to data, compared to the labels defined earlier.
\end{enumerate}


\section{Gillespie algorithm for stochastic chemical systems}
\label{append:analysis-gillespie}

To define the Gillespie algorithm \parencite{gillespieStochasticSimulationChemical2007}, consider such a system with $M$ reactions $R_{1}, \ldots , R_{j}, \ldots R_{M}$ involving $N$ species $S_{1}, \ldots , S_{i}, \ldots S_{N}$ in a fixed volume $V$ at thermal equilibrium.
Let $X_{i}(t)$ represent the number of molecules of $S_{i}$ at time $t$, and the state vector

\begin{equation}
  \mathbf{X}(t) \coloneqq [X_{1}(t), \ldots , X_{N}(t)]
  \label{eq:gillespie-statevector}
\end{equation}

thus gives the state of the system at any given time $t$.

Each reaction $R_{j}$ is described by two quantities:
\begin{enumerate}
  \item A state-change vector $\mathbf{v}_{j} \coloneqq [v_{1,j}, \ldots , v_{N,j}]$ which defines how the stoichiometry of the system changes if the reaction occurs.
        $v_{i,j}$ represents the change in the stoichiometry of $S_{i}$ when $R_{j}$ occurs.
  \item A propensity function $a_{j}(\mathbf{x})$, which gives the probability, that one $R_{j}$ reaction occurs in the volume $V$ within the following short time interval $[t, t+dt)$, given the state $\mathbf{X}(t) = \mathbf{x}$.
\end{enumerate}

The definition of the propensity function $a_{j}(\mathbf{x})$ differs depending on the reaction.
For a unimolecular reaction (\ce{S1 -> P}),

\begin{equation}
  a_{j}(\mathbf{x}) = k_{j}X_{1}
  \label{eq:gillespie-propensity-unimolecular}
\end{equation}

For a bimolecular reaction with two different species (\ce{S1 + S2 -> P}),

\begin{equation}
  a_{j}(\mathbf{x}) = \frac{k_{j}}{V} X_{1}X_{2}
  \label{eq:gillespie-propensity-bimolecular-different}
\end{equation}

And finally, for a bimolecular reaction with two molecules of the same species (\ce{S1 + S1 -> P}),

\begin{equation}
  a_{j}(\mathbf{x}) = \frac{k_{j}}{V} X_{1}(X_{1} - 1)
  \label{eq:gillespie-propensity-bimolecular-same}
\end{equation}

Where, in all cases, $k_{j}$ is the rate constant of reaction $R_{j}$.

The Gillespie algorithm aims to predict the temporal evolution of the state vector given the initial state $\mathbf{X}(t_{0}) = \mathbf{x}_{0}$.
It does so by iteratively choosing the next reaction that occurs, based on its probability, and then choosing its firing time based on a propensity function.
Combining these simulations gives a trajectory of state vectors across the time course of interest.
In detail, the direct Gillespie algorithm can be defined as stated in algorithm~\ref{alg:gillespie}:

\RestyleAlgo{ruled}
\begin{algorithm}[htbp]
  \SetAlgoLined
  \KwIn{Stochastic model (with species $S_{1}, \ldots , S_{i}, \ldots S_{N}$ and reactions $R_{1}, \ldots , R_{j}, \ldots R_{M}$, along with a state-change vector $\mathbf{v_{j}}$ and a rate constant $k_{i}$ for each reaction $R_{j}$); initial time $t_{0}$; and initial model state $\mathbf{X}(t_{0}) = \mathbf{x}_{0}$}
  \KwOut{Trajectory of state vectors $\mathbf{X}(t)$, with $t$ taking discrete values in $[t_{0}, t_{\mathrm{max}}]$}
  \While{$t < t_{\mathrm{max}}$}{
    Calculate the propensities $a_{j}(\mathbf{x})$ based on the current state $\mathbf{x}$\;
    Calculate the combined propensity $a_{0}(\mathbf{x}) = \sum_{j}a_{j}(\mathbf{x})$\;
    Generate two random numbers $r_{1}$ and $r_{2}$, both from the uniform distribution $U(0,1)$\;
    Choose the next reaction $R_{j}$, with $j$ given by the smallest integer that satisfies $\sum_{j^{\prime}}^{j}a_{j^{\prime}(\mathbf{x})} > r_{1}a_{0}(\mathbf{x})$\;
    Calculate the time to the next reaction $\tau = \frac{1}{a_{0}(\mathbf{x})}\ln(\frac{1}{r_{2}})$\;
    Simulate the next reaction by updating the state vector $\mathbf{x} \leftarrow \mathbf{x} + \mathbf{v_{j}}$ and store the new vector in $\mathbf{X}(t)$\;
    Update the time by $t \leftarrow t + \tau$ and store the new time\;
  }
  \KwRet Trajectory of state vectors $\mathbf{X}(t)$ for a vector of times $t$\;
  \caption{Direct method of the Gillespie algorithm}
  \label{alg:gillespie}
\end{algorithm}
